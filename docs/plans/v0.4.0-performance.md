# v0.4.0 Performance Release Plan — automerge-cpp

## Context

automerge-cpp v0.3.0 is functionally complete (274 tests, 10 phases done) but has significant
performance bottlenecks. The biggest issue is `visible_index_to_real()` — an O(n) linear scan
called on every list/text operation. Other bottlenecks include quadratic hash index rebuilds in
the sync path, duplicate op storage, unnecessary heap allocations, and zero parallelism.

This plan targets 2-25x improvements across all benchmark categories through:
1. **Algorithmic fixes** (Fenwick tree, hash cache, element index)
2. **Allocation reduction** (stack buffers, object pool, reserve, string dedup)
3. **Transparent internal parallelism** (thread pool powering save/load/hash/sync)
4. **Hardware acceleration** (ARM SHA-256 intrinsics, cache-line alignment)

### Design Principle: Monoid-Powered Parallelism via Execution Policies

Per the project's style guide (Ben Deane's *Composable C++*), section 2.3:

> "If the binary operation is associative with an identity, it is a monoid —
> and can be parallelized."

CRDT merge **is** a monoid:
- **Binary op:** `merge(a, b)`
- **Identity:** empty `Document{}`
- **Associative:** `merge(merge(a, b), c) == merge(a, merge(b, c))`
- **Commutative:** `merge(a, b) == merge(b, a)`
- **Idempotent:** `merge(a, a) == a`

Therefore all parallelism uses **`std::execution::par`** with standard algorithms:

```
// Parallel merge of N documents — monoid reduce
auto merged = std::reduce(std::execution::par,
    docs.begin(), docs.end(), Document{},
    [](auto acc, const auto& d) { acc.merge(d); return acc; });

// Parallel save — standard transform
std::transform(std::execution::par, docs.begin(), docs.end(), saved.begin(),
    [](const auto& d) { return d.save(); });

// Parallel mutate — standard for_each
std::for_each(std::execution::par, docs.begin(), docs.end(),
    [](auto& d) { d.transact([](auto& tx) { tx.put(root, "v", 2); }); });
```

**No custom thread pool.** No wrapper class. No parallel free functions. The runtime's thread
pool handles scheduling (TBB on Linux, GCD on macOS, ConcRT on Windows).

**Document is thread-safe by default** using `std::shared_mutex` internally:
- Read methods (`get`, `text`, `keys`, `values`, `length`, `save`, etc.) acquire a **shared
  lock** — N readers run concurrently.
- Write methods (`transact`, `merge`, `apply_changes`, `receive_sync_message`) acquire an
  **exclusive lock**.
- `read(fn)` / `write(fn)` scoped accessors hold the lock for multi-step operations.
- **1 document with 120 threads works.** 1000 documents with 30 threads works.

Internal parallelism in `save()` / `load()` also uses `std::execution::par` on change chunks,
column compression, and SHA-256 hashing.

- **Fork-per-thread breaks semantics.** Forking creates a new ActorId, which changes OpIds,
  change hashes, and conflict resolution order. Not equivalent to sequential operations.
- **Internal parallelism is invisible.** `save()` returns the same bytes whether it used 1
  core or 30. `load()` produces the same document.

**Target platform for local testing:** Apple M3 Max (16 cores) / 30-core server, macOS/Linux,
Apple Clang / GCC, Release build.

---

## Phase 11A: Core Data Structure Optimizations

### 11A.1 — Remove dead `op_log` field
- **Files:** `src/doc_state.hpp`, `src/transaction.cpp`, `src/document.cpp`
- **What:** Remove `std::vector<Op> op_log` from `DocState` and all `push_back` calls. It is
  never read — `change_history` stores all ops inside `Change.operations`.
- **Impact:** ~10-15% faster mutations, ~40% memory reduction for op storage.
- **Risk:** LOW — dead code removal.

### 11A.2 — Replace `std::map<ObjId, ObjectState>` with `std::unordered_map`
- **File:** `src/doc_state.hpp`
- **What:** Change `objects` from `std::map` to `std::unordered_map`. `std::hash<ObjId>`
  already exists in `types.hpp`.
- **Impact:** O(log n) → O(1) object lookup. ~15% faster map_put, ~30% faster map_get.
- **Risk:** LOW — hash already implemented, no code depends on iteration order.

### 11A.3 — Cache the change hash index
- **File:** `src/doc_state.hpp`
- **What:** Add a cached `std::unordered_map<ChangeHash, std::size_t>` to `DocState`. Rebuild
  incrementally (changes are append-only). Eliminate the O(n) `change_hash_index()` rebuild on
  every sync/time-travel call. Fix `has_change_hash()` which currently rebuilds the full index
  per call (making `get_missing_deps` quadratic).
- **API:** Replace `change_hash_index()` with `ensure_hash_index()` that lazily builds and
  caches. All callers use the cached version. Invalidate by tracking
  `cached_hash_index_size_` — if `change_history.size()` has grown, append new entries.
- **Impact:** sync_full_round_trip 4-5x faster, get_at 2x faster.
- **Risk:** LOW — cache invalidation is trivial (append-only data).

### 11A.4 — Fenwick tree for `visible_index_to_real`
- **File:** `src/doc_state.hpp`
- **What:** Add a `FenwickTree` (Binary Indexed Tree) to `ObjectState` for O(log n)
  visible-index-to-real-index conversion via `find_kth` (binary lifting). Maintain on insert
  (+1) and delete (flip visibility). Replace the O(n) linear scan at line 193.
- **Design:**
  ```
  class FenwickTree {
      std::vector<int> tree_;  // tree_[i] = partial sum of visible elements
  public:
      void insert(std::size_t pos, int val);   // insert element, shift tree
      void update(std::size_t pos, int delta); // toggle visibility (+1/-1)
      auto prefix_sum(std::size_t pos) const -> int;
      auto find_kth(int k) const -> std::size_t; // O(log n) binary lifting
      auto total() const -> int;
  };
  ```
- **Integration:** `ObjectState` gains `FenwickTree visibility_tree`. Updated by:
  - `list_insert()`: insert +1 at real_idx
  - `list_delete()`: update -1 at real_idx
  - `apply_op(insert/splice_text)`: insert +1 at rga_pos
  - `apply_op(del)`: update -1 at element position
- **Debug validation:** In debug builds, assert `find_kth` result matches linear scan.
- **Impact:** list/text operations 7-25x faster. This is the single highest-impact optimization.
- **Risk:** LOW-MEDIUM — well-understood data structure, validated against linear scan.

### 11A.5 — OpId-to-position index for RGA merge
- **File:** `src/doc_state.hpp`
- **What:** Add `std::unordered_map<OpId, std::size_t> element_index` to `ObjectState` mapping
  `insert_id → real_index`. Replace linear search in `find_rga_position` (line 380) with O(1)
  hash lookup. Maintain on insert (shift all indices >= insertion point).
- **Impact:** merge 1.5-2x faster, critical for large document merges.
- **Risk:** MEDIUM — index maintenance on vector insert requires careful handling.
- **Depends on:** 11A.4 (both modify list element management).

---

## Phase 11B: Memory & Allocation Optimizations

### 11B.1 — Bloom filter stack-allocated probe array
- **File:** `src/sync/bloom_filter.hpp`
- **What:** Change `get_probes()` return from `std::vector<uint32_t>` to
  `std::array<uint32_t, 7>`. Eliminates heap allocation per bloom filter check.
- **Impact:** 5-10% faster sync operations.
- **Risk:** LOW.

### 11B.2 — String copy elimination in Transaction
- **File:** `src/transaction.cpp`
- **What:** In `put()`, `put_object()`, `delete_key()`, `increment()`, `mark()` — create
  `std::string` from key once and move it through the call chain, instead of creating 3 copies
  per operation. Change `map_pred()`, `map_put()`, `map_delete()`, `counter_increment()` to
  accept `const std::string&` or take by move where the string is consumed.
- **Impact:** ~25% faster map_put (saves 2 heap allocations per put).
- **Risk:** LOW — pure refactoring.

### 11B.3 — Serialization buffer pre-sizing
- **Files:** `src/document.cpp`, `src/storage/change_chunk.hpp`, `src/storage/columns/raw_column.hpp`
- **What:** Add `reserve()` calls with size heuristics before building output buffers.
  E.g., `body.reserve(128 + change_history.size() * 256)` in `save()`.
  In `serialize_change_body()`: `body.reserve(64 + ops.size() * 32)`.
  In `write_raw_columns()`: `output.reserve(output.size() + total_column_bytes)`.
- **Impact:** save 1.5-1.7x faster, save_large 1.7x faster.
- **Risk:** LOW.

### 11B.4 — SHA-256 stack-allocated padding buffer
- **File:** `src/crypto/sha256.hpp`
- **What:** Replace heap-allocated `std::vector<std::byte>` padding with a stack buffer.
  Use `std::array<std::byte, 256>` for inputs up to 247 bytes (covers typical change hashing).
  Fall back to heap for larger inputs.
- **Impact:** 10-20% faster SHA-256, benefits all save/load/commit/sync paths.
- **Risk:** LOW — output verified by existing 7 NIST test vectors.

### 11B.5 — Reserve-based pre-allocation throughout
- **Files:** `src/document.cpp`, `src/doc_state.hpp`
- **What:** Add `reserve()` in `ops_to_patches()`, `build_actor_table()`, `all_change_hashes()`,
  `get_changes_since()`, `get_missing_deps()`, `get_changes_by_hash()`, `changes_visible_at()`,
  and other vector-building functions where size is known or estimable.
- **Impact:** 2-5% cumulative improvement.
- **Risk:** LOW.

### 11B.6 — Object pool for Op and ListElement
- **New file:** `src/pool.hpp`
- **What:** Implement a typed arena/object pool (`ObjectPool<T>`) that pre-allocates blocks of
  frequently created/destroyed objects. Use it for `Op` in the transaction hot path.
- **Design:**
  ```
  template <typename T, std::size_t BlockSize = 256>
  class ObjectPool {
      std::vector<std::unique_ptr<std::array<T, BlockSize>>> blocks_;
      std::vector<T*> free_list_;
  public:
      auto acquire() -> T*;        // O(1) amortized
      void release(T* obj);        // O(1)
      void reserve(std::size_t n); // pre-allocate n slots
  };
  ```
- **Usage in `Transaction`:** Instead of `pending_ops_.push_back(std::move(op))` allocating
  from the global heap on every operation, acquire from the pool. On `commit()`, ops move
  into the `Change` and pool slots are released.
- **Impact:** Eliminates per-operation heap allocation/deallocation churn. 15-30% faster for
  bulk transaction workloads (map_put_batch, text_splice_bulk).
- **Risk:** LOW-MEDIUM — pool is simple, integration touches hot paths. Pool owned by `DocState`.

---

## Phase 11C: Thread Pool & Transparent Internal Parallelism

### Architecture

All parallelism is **internal to the library**. The `Document` API is unchanged. A thread pool
is used as an implementation detail to accelerate save, load, hashing, and sync. The pool is a
process-global singleton sized to `std::thread::hardware_concurrency()` (16-30 cores).

Parallelism is only engaged when the workload justifies it (e.g., >32 items to process).
Below that threshold, the sequential path runs with zero overhead.

### 11C.1 — Thread-safe Document + execution policy infrastructure
- **Files:** `src/doc_state.hpp`, `include/automerge-cpp/document.hpp`, `src/document.cpp`,
  `CMakeLists.txt`
- **What:** Make `Document` thread-safe and enable `std::execution::par` throughout.

  **Part A — `std::shared_mutex` in DocState:**
  Add `mutable std::shared_mutex mutex_` to `DocState`. All existing `Document` methods
  acquire the appropriate lock:
  - Read methods (`get`, `text`, `keys`, `values`, `length`, `save`, `get_heads`,
    `generate_sync_message`, `cursor`, `resolve_cursor`, `marks`, all `*_at` methods):
    `std::shared_lock lock{state_->mutex_}` — N concurrent readers.
  - Write methods (`transact`, `transact_with_patches`, `merge`, `apply_changes`,
    `receive_sync_message`, `set_actor_id`):
    `std::unique_lock lock{state_->mutex_}` — exclusive access.

  **Part B — Scoped accessors:**
  ```
  // Hold shared lock for multi-step reads (consistent snapshot)
  template <typename F>
  auto read(F&& fn) const -> std::invoke_result_t<F, const Document&>;

  // Hold exclusive lock for multi-step writes (atomic to readers)
  template <typename F>
  auto write(F&& fn) -> std::invoke_result_t<F, Document&>;
  ```

  **Part C — CMake: link execution policy backend:**
  ```
  # TBB for GCC parallel algorithms; GCD used automatically on macOS
  find_package(TBB QUIET)
  if(TBB_FOUND)
      target_link_libraries(automerge-cpp PRIVATE TBB::tbb)
  endif()
  ```

- **Design rationale:** No custom thread pool. `std::execution::par` uses the platform's
  thread pool (TBB on Linux, GCD on macOS, ConcRT on Windows). Users parallelize across
  documents with `std::transform(par, ...)`, `std::for_each(par, ...)`,
  `std::reduce(par, ...)`. Internal parallelism in save/load also uses execution policies.
- **Impact:** Foundation for all parallel work. 1 doc with 120 threads works.
  1000 docs with 30 threads works via standard algorithms.
- **Risk:** LOW — `shared_mutex` is well-understood; execution policies are standard C++17/20.

### 11C.2 — Parallel change serialization during save
- **File:** `src/document.cpp`
- **What:** In `Document::save()`, serialize each change body in parallel using
  `std::transform(std::execution::par, ...)`:
  ```
  auto bodies = std::vector<std::vector<std::byte>>(state_->change_history.size());
  std::transform(std::execution::par,
      state_->change_history.begin(), state_->change_history.end(),
      bodies.begin(),
      [&actor_table](const Change& change) {
          return storage::serialize_change_body(change, actor_table);
      }
  );
  // Sequential assembly of output from bodies
  ```
  Each `serialize_change_body()` is fully independent — reads the `Change` and
  `actor_table` (both const) and produces a byte vector. No shared mutable state.
- **Impact:** save with N changes: up to min(N, cores)x faster. For 1000-change documents
  on a 30-core machine: ~25-30x faster serialization.
- **Risk:** LOW — each call is pure (reads const data, writes to independent output).

### 11C.3 — Parallel DEFLATE compression during save
- **File:** `src/storage/change_chunk.hpp`
- **What:** In `serialize_change_body()`, after column encoding, compress columns >256 bytes
  in parallel using `std::for_each(std::execution::par, ...)`:
  ```
  // Identify compressible columns, transform in parallel
  auto compress_indices = compressible_columns
      | std::views::filter([](const auto& col) { return col.data.size() > deflate_threshold; })
      | std::ranges::to<std::vector>();

  std::for_each(std::execution::par,
      compress_indices.begin(), compress_indices.end(),
      [&columns](std::size_t i) {
          if (auto compressed = deflate_compress(columns[i].data)) {
              columns[i].data = std::move(*compressed);
              columns[i].spec.set_deflate(true);
          }
      }
  );
  ```
  Each `z_stream` is independent. zlib is thread-safe for independent streams.
- **Impact:** Per-change serialization: up to 14 columns compressed in parallel.
  Combined with 11C.2 (parallel across changes): multiplicative speedup.
- **Risk:** LOW — each compression is independent.

### 11C.4 — Parallel DEFLATE decompression during load
- **File:** `src/storage/change_chunk.hpp`
- **What:** In `parse_change_chunk()`, decompress all compressed columns in parallel.
  Same `std::for_each(par, ...)` pattern as 11C.3 but for `deflate_decompress()`.
- **Impact:** load 2-4x faster per change chunk for compressed documents.
- **Risk:** LOW — same pattern as 11C.3.

### 11C.5 — Parallel change chunk parsing during load
- **File:** `src/document.cpp`
- **What:** In `parse_v2()`, after reading chunk byte ranges, parse all change chunks
  in parallel using `std::transform(std::execution::par, ...)`:
  ```
  auto changes = std::vector<std::optional<Change>>(chunk_spans.size());
  std::transform(std::execution::par,
      chunk_spans.begin(), chunk_spans.end(), changes.begin(),
      [&actor_table](std::span<const std::byte> span) {
          return storage::parse_change_chunk(span, actor_table);
      }
  );
  // Sequential: apply changes (CRDT correctness requires causal ordering)
  ```
- **Impact:** Load with N changes: up to min(N, cores)x faster for the parsing phase.
- **Risk:** LOW — each parse is pure.

### 11C.6 — Parallel SHA-256 hash computation
- **File:** `src/doc_state.hpp`
- **What:** When computing hashes for multiple changes (cache rebuild, `all_change_hashes()`,
  `get_changes_since()`), hash in parallel using `std::transform(std::execution::par, ...)`:
  ```
  auto hashes = std::vector<ChangeHash>(change_history.size());
  std::transform(std::execution::par,
      change_history.begin(), change_history.end(), hashes.begin(),
      [](const Change& c) { return compute_change_hash(c); }
  );
  ```
  Each SHA-256 computation is completely independent.
- **Impact:** For 1000 changes on 30 cores: ~25-30x faster hash computation.
- **Risk:** LOW — each hash is pure.

### 11C.7 — Parallel column encoding during save
- **File:** `src/storage/columns/change_op_columns.hpp`
- **What:** In `encode_change_ops()`, the op iteration loop (feeding encoders) is
  sequential (encoder state depends on previous ops). But the 14 `finish()` + `take()`
  calls are independent. When combined with 11C.2, each change's ENTIRE column encoding
  is already running on its own execution-policy-managed thread. The 14 encoders' `take()`
  calls can also run in parallel via `std::for_each(par, ...)`.
- **Impact:** Marginal within a single change, significant for many-change documents.
- **Risk:** LOW.

### 11C.8 — Parallel bloom filter construction
- **File:** `src/sync/bloom_filter.hpp`
- **What:** When building a bloom filter from many hashes, compute probe positions in
  parallel. Use `std::atomic<uint8_t>` for the bit array during parallel construction:
  ```
  auto atomic_bits = std::vector<std::atomic<uint8_t>>(byte_count);
  std::for_each(std::execution::par, hashes.begin(), hashes.end(),
      [&atomic_bits](const ChangeHash& h) {
          auto probes = get_probes(h);
          for (auto p : probes) {
              atomic_bits[p >> 3].fetch_or(1u << (p & 7), std::memory_order_relaxed);
          }
      }
  );
  ```
- **Impact:** For documents with 1000+ changes, bloom construction is 10-25x faster.
- **Risk:** LOW-MEDIUM — atomic ops are well-understood; relaxed ordering sufficient for OR.

### 11C.9 — (Removed — absorbed into 11C.1)

The thread-safe Document design and `read()`/`write()` scoped accessors are part of 11C.1.
No separate `ThreadSafeDocument` wrapper class. Users parallelize across documents with
standard algorithms: `std::transform(par, ...)`, `std::for_each(par, ...)`,
`std::reduce(par, ...)`. Merge is a monoid, so `std::reduce(par, ...)` parallelizes it.

---

## Phase 11D: Hardware Acceleration

### 11D.1 — ARM Crypto Extensions for SHA-256
- **File:** `src/crypto/sha256.hpp`
- **What:** Add compile-time-detected ARM SHA-256 hardware implementation using NEON
  intrinsics (`vsha256hq_u32`, `vsha256h2q_u32`, `vsha256su0q_u32`, `vsha256su1q_u32`).
  Keep software fallback for x86/MSVC/FreeBSD. Optionally add x86 SHA-NI path.
- **Detection:** `#if defined(__ARM_FEATURE_SHA2) || (defined(__aarch64__) && defined(__APPLE__))`
- **Impact:** SHA-256 5-10x faster on Apple Silicon / ARMv8.2+.
  Combined with 11C.6 (parallel hashing): multiplicative — 30 cores x 5-10x per core.
- **Risk:** MEDIUM — platform-specific intrinsics, must maintain software fallback.
  Validated by existing 7 SHA-256 NIST test vectors.
- **Depends on:** 11B.4.

### 11D.2 — Cache-line alignment for ObjectState
- **File:** `src/doc_state.hpp`
- **What:** Add `alignas(64)` to `ObjectState` to prevent false sharing when multiple
  objects are accessed from different threads during parallel operations. Add `alignas(64)`
  to `FenwickTree` internal vector as well.
- **Impact:** 10-20% faster when objects are accessed in parallel (e.g., during parallel
  change application or parallel bloom filter probes that touch different objects).
- **Risk:** LOW — alignment only, no structural changes.
- **Depends on:** 11A.4.

---

## Phase 11E: Benchmark Expansion & Validation

### 11E.1 — Large document benchmarks
- **File:** `benchmarks/placeholder_benchmark.cpp`
- **What:** Add benchmarks for:
  - 10K/100K element lists (catches O(n^2) in Fenwick tree and element index)
  - 10K-key maps
  - Large merges (100+ changes from multiple actors)
  - Large save/load (1000-change documents)
  - Large sync (documents with significant divergence)
  - Parallel save throughput (how well does it scale with core count?)
  - Parallel load throughput

### 11E.2 — Multi-threaded benchmarks
- **File:** `benchmarks/placeholder_benchmark.cpp`
- **What:** Add benchmarks measuring:
  - N threads doing concurrent reads on a single Document
  - Parallel save vs sequential save (varying change count: 10, 100, 1000)
  - Parallel load vs sequential load
  - std::reduce(par) merge (100 docs) vs sequential reduce
  - std::transform(par) save/load (1000 docs) vs sequential
  - Object pool throughput (acquire/release cycles)

### 11E.3 — Before/after comparison
- **What:** Save v0.3.0 baseline as JSON, compare with v0.4.0 using
  `google-benchmark --benchmark_format=json`.

### 11E.4 — Regression verification
- **What:** All 274 existing tests pass after each sub-phase. Add debug-mode assertions to
  validate Fenwick tree against linear scan, hash cache against full rebuild, element index
  against linear search. Add thread safety tests for concurrent Document access.

---

## Implementation Order

| Step | Item | Effort | Cumulative Impact |
|------|------|--------|-------------------|
| 1 | 11A.1 Remove op_log | 10 min | 10-15% mutations |
| 2 | 11A.2 unordered_map | 10 min | +15-30% lookups |
| 3 | 11B.1 Bloom stack array | 10 min | +5-10% sync |
| 4 | 11B.2 String copy elim | 20 min | +25% map_put |
| 5 | 11A.3 Hash cache | 30 min | +4-5x sync, +2x time travel |
| 6 | 11B.3 Serialization reserve | 15 min | +1.5x save |
| 7 | 11B.4 SHA-256 stack buf | 30 min | +10-20% hashing |
| 8 | 11B.5 Reserve throughout | 15 min | +2-5% cumulative |
| 9 | 11B.6 Object pool | 1 hr | +15-30% bulk mutations |
| 10 | 11A.4 Fenwick tree | 2 hr | +7-25x list/text |
| 11 | 11A.5 Element index | 1 hr | +1.5-2x merge |
| 12 | 11C.1 Thread-safe Document + execution policies | 1.5 hr | Foundation for parallelism |
| 13 | 11C.2 Parallel change serialization (par) | 45 min | Up to 30x save (many changes) |
| 14 | 11C.3 Parallel column compression (par) | 30 min | +2-4x per-change save |
| 15 | 11C.4 Parallel column decompression (par) | 30 min | +2-4x per-change load |
| 16 | 11C.5 Parallel change chunk parsing (par) | 45 min | Up to 30x load parsing |
| 17 | 11C.6 Parallel SHA-256 hashing (par) | 30 min | Up to 30x hash computation |
| 18 | 11C.7 Parallel column encoding (par) | 30 min | Additional save speedup |
| 19 | 11C.8 Parallel bloom construction (par) | 30 min | +10-25x bloom for large docs |
| 21 | 11D.1 ARM Crypto SHA-256 | 2 hr | +5-10x per-core SHA-256 |
| 22 | 11D.2 Cache alignment | 30 min | +10-20% parallel access |
| 23 | 11E.1-4 Benchmarks + tests | 1.5 hr | Validation |

---

## Parallelism Map: What Runs on 30 Cores

All parallelism uses `std::execution::par` — the platform's thread pool handles scheduling.

| Operation | Sequential Part | Parallel Part (`par`) | Max Speedup (30 cores) |
|-----------|----------------|----------------------|----------------------|
| `save()` | Header assembly | `std::transform(par)` change bodies (11C.2) | 25-30x |
| `save()` (per change) | Op loop for encoders | `std::for_each(par)` column compress (11C.3) | 14x |
| `load()` | Header parse | `std::transform(par)` chunk parsing (11C.5) | 25-30x |
| `load()` (per chunk) | Column decode | `std::for_each(par)` decompress (11C.4) | 14x |
| `load()` | — | `std::transform(par)` hash cache rebuild (11C.6) | 25-30x |
| `all_change_hashes()` | — | `std::transform(par)` SHA-256 (11C.6) | 25-30x |
| `generate_sync_message()` | BFS traversal | `std::for_each(par)` bloom build (11C.8) | 10-25x |
| SHA-256 (per call) | — | ARM intrinsics (11D.1) | 5-10x per core |
| **User: N docs** | — | `std::transform(par)` save/load/get | up to Nx |
| **User: merge N docs** | — | `std::reduce(par)` — **merge is a monoid** | up to Nx |

**Combined example — save a 1000-change document on 30 cores:**
- Sequential: serialize 1000 changes x (column encode + compress) = ~1000 x 1ms = 1s
- Parallel: 1000 changes / 30 cores = ~34 batches x 1ms = 34ms → **~30x speedup**

**Combined example — merge 100 peer documents on 30 cores:**
- Sequential: `std::reduce(seq, peers, empty_doc, merge)` = 100 merges × ~4μs = 400μs
- Parallel: `std::reduce(par, peers, empty_doc, merge)` — tree reduction on 30 cores
  = ~log2(100) × merge_time ≈ **~15x speedup**

**Combined example — load a 1000-change compressed document on 30 cores:**
- Parallel parse: `std::transform(par)` on 1000 chunks → **~30x faster parsing**
- Sequential apply: unchanged (CRDT correctness)
- Parallel hash cache: `std::transform(par)` on 1000 hashes → **~30x faster hashing**

---

## New Files

| File | Description |
|------|-------------|
| `src/pool.hpp` | Typed object pool for Op, ListElement |

---

## Projected Results (v0.3.0 → v0.4.0)

| Benchmark | v0.3.0 | v0.4.0 (est.) | Speedup |
|-----------|--------|---------------|---------|
| map_put (single) | 953 K ops/s | ~1.4 M ops/s | 1.5x |
| map_put_batch/1000 | 3.44 M ops/s | ~5.5 M ops/s | 1.6x |
| map_get | 28.5 M ops/s | ~50 M ops/s | 1.8x |
| list_insert_append | 15.2 K ops/s | ~200 K ops/s | 13x |
| list_insert_front | 14.6 K ops/s | ~200 K ops/s | 14x |
| list_get (1000 elem) | 4.48 M ops/s | ~33 M ops/s | 7x |
| text_splice_bulk (100 chars) | 27 K chars/s | ~670 K chars/s | 25x |
| save (100 keys, 1 change) | 10.4 K ops/s | ~18 K ops/s | 1.7x |
| save (1000 changes) | — | — | ~25-30x |
| load (100 keys, 1 change) | 38.2 K ops/s | ~67 K ops/s | 1.7x |
| load (1000 changes) | — | — | ~20-25x |
| save_large (1000 items) | 1.38 KiB/s | ~35 KiB/s | 25x |
| merge (10+10 puts) | 248.7 K ops/s | ~500 K ops/s | 2x |
| sync_full_round_trip | 4.28 K ops/s | ~20 K ops/s | 4.7x |
| cursor_resolve | 6.04 M ops/s | ~40 M ops/s | 6.6x |
| **NEW: parallel save (1000 changes, 30 cores)** | N/A | benchmark | — |
| **NEW: parallel load (1000 changes, 30 cores)** | N/A | benchmark | — |
| **NEW: concurrent reads (Document, 120 threads)** | N/A | benchmark | — |
| **NEW: std::reduce(par) merge (100 docs, 30 cores)** | N/A | benchmark | — |

---

## Verification

After each sub-phase:
```bash
cmake --build build && ctest --test-dir build --output-on-failure
```

After all phases, run release benchmarks:
```bash
cmake -B build-release -DCMAKE_BUILD_TYPE=Release \
    -DAUTOMERGE_CPP_BUILD_BENCHMARKS=ON \
    -DAUTOMERGE_CPP_BUILD_TESTS=ON
cmake --build build-release
./build-release/benchmarks/automerge_cpp_benchmarks
```

Compare against v0.3.0 baseline numbers documented in `docs/benchmark-results.md`.

---

## Critical Files

| File | Optimizations |
|------|--------------|
| `src/doc_state.hpp` | 11A.1-5, 11B.5, 11D.2 (central target) |
| `src/transaction.cpp` | 11A.1, 11B.2, 11B.6 |
| `src/document.cpp` | 11A.1, 11B.3, 11B.5, 11C.2, 11C.5 |
| `src/crypto/sha256.hpp` | 11B.4, 11D.1 |
| `src/storage/change_chunk.hpp` | 11B.3, 11C.3, 11C.4 |
| `src/storage/columns/change_op_columns.hpp` | 11C.7 |
| `src/sync/bloom_filter.hpp` | 11B.1, 11C.8 |
| `src/storage/columns/raw_column.hpp` | 11B.3 |
| `src/pool.hpp` | 11B.6 (NEW) |
| `benchmarks/placeholder_benchmark.cpp` | 11E.1-2 |
| `docs/plans/roadmap.md` | Update with Phase 11 |
