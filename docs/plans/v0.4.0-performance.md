# v0.4.0 Performance Release Plan — automerge-cpp

## Context

automerge-cpp v0.3.0 is functionally complete (274 tests, 10 phases done) but has significant
performance bottlenecks. The biggest issue is `visible_index_to_real()` — an O(n) linear scan
called on every list/text operation. Other bottlenecks include quadratic hash index rebuilds in
the sync path, duplicate op storage, unnecessary heap allocations, and no parallelism or
hardware acceleration. This plan targets 2-25x improvements across all benchmark categories,
with the largest gains in list/text operations.

v0.4.0 introduces proper multi-threading infrastructure: a reusable thread pool, object pools
for hot-path allocations, a `ThreadSafeDocument` wrapper for concurrent access, and parallel
pipelines for save/load/merge/sync operations.

**Target platform for local testing:** Apple M3 Max (16 cores), macOS, Apple Clang, Release build.

---

## Phase 11A: Core Data Structure Optimizations

### 11A.1 — Remove dead `op_log` field
- **Files:** `src/doc_state.hpp`, `src/transaction.cpp`, `src/document.cpp`
- **What:** Remove `std::vector<Op> op_log` from `DocState` and all `push_back` calls. It is
  never read — `change_history` stores all ops inside `Change.operations`.
- **Impact:** ~10-15% faster mutations, ~40% memory reduction for op storage.
- **Risk:** LOW — dead code removal.

### 11A.2 — Replace `std::map<ObjId, ObjectState>` with `std::unordered_map`
- **File:** `src/doc_state.hpp`
- **What:** Change `objects` from `std::map` to `std::unordered_map`. `std::hash<ObjId>`
  already exists in `types.hpp`.
- **Impact:** O(log n) → O(1) object lookup. ~15% faster map_put, ~30% faster map_get.
- **Risk:** LOW — hash already implemented, no code depends on iteration order.

### 11A.3 — Cache the change hash index
- **File:** `src/doc_state.hpp`
- **What:** Add a cached `std::unordered_map<ChangeHash, std::size_t>` to `DocState`. Rebuild
  incrementally (changes are append-only). Eliminate the O(n) `change_hash_index()` rebuild on
  every sync/time-travel call. Fix `has_change_hash()` which currently rebuilds the full index
  per call (making `get_missing_deps` quadratic).
- **Impact:** sync_full_round_trip 4-5x faster, get_at 2x faster.
- **Risk:** LOW — cache invalidation is trivial (append-only data).

### 11A.4 — Fenwick tree for `visible_index_to_real`
- **File:** `src/doc_state.hpp`
- **What:** Add a `FenwickTree` (Binary Indexed Tree) to `ObjectState` for O(log n)
  visible-index-to-real-index conversion via `find_kth` (binary lifting). Maintain on insert
  (+1) and delete (flip visibility). Replace the O(n) linear scan at line 193.
- **Impact:** list/text operations 7-25x faster. This is the single highest-impact optimization.
- **Risk:** LOW-MEDIUM — well-understood data structure, can validate against linear scan in
  debug mode.

### 11A.5 — OpId-to-position index for RGA merge
- **File:** `src/doc_state.hpp`
- **What:** Add `std::unordered_map<OpId, std::size_t> element_index` to `ObjectState` mapping
  `insert_id → real_index`. Replace linear search in `find_rga_position` (line 380) with O(1)
  hash lookup. Maintain on insert (shift indices >= insertion point).
- **Impact:** merge 1.5-2x faster, critical for large document merges.
- **Risk:** MEDIUM — index maintenance on vector insert requires careful handling.
- **Depends on:** 11A.4 (both modify list element management).

---

## Phase 11B: Memory & Allocation Optimizations

### 11B.1 — Bloom filter stack-allocated probe array
- **File:** `src/sync/bloom_filter.hpp`
- **What:** Change `get_probes()` return from `std::vector<uint32_t>` to
  `std::array<uint32_t, 7>`. Eliminates heap allocation per bloom filter check.
- **Impact:** 5-10% faster sync operations.
- **Risk:** LOW.

### 11B.2 — String copy elimination in Transaction
- **File:** `src/transaction.cpp`
- **What:** In `put()`, `put_object()`, `delete_key()`, `increment()`, `mark()` — create
  `std::string` from key once and move it through the call chain, instead of creating 3 copies
  per operation.
- **Impact:** ~25% faster map_put (saves 2 heap allocations per put).
- **Risk:** LOW — pure refactoring.

### 11B.3 — Serialization buffer pre-sizing
- **Files:** `src/document.cpp`, `src/storage/change_chunk.hpp`, `src/storage/columns/raw_column.hpp`
- **What:** Add `reserve()` calls with size heuristics before building output buffers.
  E.g., `body.reserve(128 + change_history.size() * 256)` in `save()`.
- **Impact:** save 1.5-1.7x faster, save_large 1.7x faster.
- **Risk:** LOW.

### 11B.4 — SHA-256 stack-allocated padding buffer
- **File:** `src/crypto/sha256.hpp`
- **What:** Replace heap-allocated `std::vector<std::byte>` padding with a stack buffer
  (256 bytes covers typical inputs). Fall back to heap for large inputs.
- **Impact:** 10-20% faster SHA-256, benefits all save/load/commit/sync paths.
- **Risk:** LOW — output verified by existing 7 NIST test vectors.

### 11B.5 — Reserve-based pre-allocation throughout
- **Files:** `src/document.cpp`, `src/doc_state.hpp`
- **What:** Add `reserve()` in `ops_to_patches()`, `build_actor_table()`, and other
  vector-building functions where size is known.
- **Impact:** 2-5% cumulative improvement.
- **Risk:** LOW.

### 11B.6 — Object pool for Op and ListElement
- **New file:** `src/pool.hpp`
- **What:** Implement a typed object pool (`ObjectPool<T>`) that pre-allocates blocks of
  frequently created/destroyed objects. Use it for `Op`, `ListElement`, `MapEntry`, and
  `Change` in the hot paths.
- **Design:**
  ```
  template <typename T, std::size_t BlockSize = 256>
  class ObjectPool {
      std::vector<std::unique_ptr<std::array<T, BlockSize>>> blocks_;
      std::vector<T*> free_list_;
  public:
      auto acquire() -> T*;        // O(1) amortized
      void release(T* obj);        // O(1)
      void reserve(std::size_t n); // pre-allocate n objects
  };
  ```
- **Usage in `Transaction`:** Instead of `pending_ops_.push_back(std::move(op))` allocating
  from the global heap on every operation, acquire from the pool. On `commit()`, ops are moved
  into the `Change` and pool slots are released.
- **Usage in `DocState::apply_op`:** `ListElement` insertions acquire from the pool instead of
  triggering `vector::insert` reallocation on every insert.
- **Impact:** Eliminates per-operation heap allocation/deallocation churn. 15-30% faster for
  bulk transaction workloads (map_put_batch, text_splice_bulk).
- **Risk:** LOW-MEDIUM — pool is simple, but integration touches hot paths. Pool must be
  owned by `DocState` for lifetime management.

---

## Phase 11C: Multi-threading Infrastructure & Parallel Operations

### 11C.1 — Thread pool (`src/thread_pool.hpp`)
- **New file:** `src/thread_pool.hpp`
- **What:** Implement a lightweight, header-only thread pool using C++23 primitives. This is
  the foundation for all parallel operations in Phases 11C.2-11C.7.
- **Design:**
  ```
  class ThreadPool {
      std::vector<std::jthread> workers_;
      std::queue<std::move_only_function<void()>> tasks_;
      std::mutex mutex_;
      std::condition_variable_any cv_;
      std::stop_source stop_;
  public:
      explicit ThreadPool(std::size_t num_threads = std::thread::hardware_concurrency());
      ~ThreadPool();  // signals stop, joins all workers

      // Submit a task and get a future for the result
      template <typename F>
      auto submit(F&& f) -> std::future<std::invoke_result_t<F>>;

      // Submit multiple tasks and wait for all to complete
      template <typename Iter>
      void parallel_for(Iter begin, Iter end, auto&& fn);

      auto thread_count() const -> std::size_t;
  };
  ```
- **Key features:**
  - Uses `std::jthread` with cooperative `std::stop_token` for clean shutdown
  - `std::move_only_function` (C++23) for type-erased task storage (no std::function overhead)
  - Work-stealing not needed at this scale — simple shared queue with mutex
  - `parallel_for` helper for data-parallel loops
  - Default thread count = `std::hardware_concurrency()` (16 on M3 Max)
- **Ownership:** Singleton per process or one per `Document`. Start with a global singleton
  accessed via `ThreadPool::instance()`.
- **Impact:** Foundation for all parallel work below.
- **Risk:** LOW — `std::jthread` handles lifetime; `std::move_only_function` avoids
  allocation for small callables.

### 11C.2 — Parallel DEFLATE compression during save
- **File:** `src/storage/change_chunk.hpp`
- **What:** After column encoding, submit compression of each column (>256 bytes) to the
  thread pool. Wait for all futures before assembling the output.
  ```
  auto& pool = ThreadPool::instance();
  auto futures = std::vector<std::future<std::optional<std::vector<std::byte>>>>{};
  for (auto& col : columns) {
      if (col.data.size() > deflate_threshold) {
          futures.push_back(pool.submit([&col] {
              return deflate_compress(col.data);
          }));
      }
  }
  // ... collect results
  ```
- **Impact:** save_large 2-4x faster when compression dominates. Up to 14 columns compressed
  in parallel.
- **Risk:** LOW — each `z_stream` is independent. zlib is thread-safe for independent streams.
- **Depends on:** 11C.1 (thread pool).

### 11C.3 — Parallel DEFLATE decompression during load
- **File:** `src/storage/change_chunk.hpp`
- **What:** Same pattern as 11C.2 for decompression in `parse_change_chunk()`. Submit each
  compressed column to the thread pool for parallel decompression.
- **Impact:** load 2-4x faster for compressed documents.
- **Depends on:** 11C.1 (thread pool).

### 11C.4 — Parallel SHA-256 hash computation
- **File:** `src/doc_state.hpp`
- **What:** When rebuilding the hash cache (11A.3) or computing hashes during load, use the
  thread pool to hash multiple changes concurrently:
  ```
  auto& pool = ThreadPool::instance();
  auto hashes = std::vector<ChangeHash>(changes.size());
  pool.parallel_for(0, changes.size(), [&](std::size_t i) {
      hashes[i] = compute_change_hash(changes[i]);
  });
  ```
- **Impact:** Large document load with 1000+ changes: 2-4x faster hash computation on M3 Max
  (16 cores doing independent SHA-256).
- **Depends on:** 11C.1 (thread pool), 11A.3 (hash cache).

### 11C.5 — Parallel change application during merge
- **File:** `src/document.cpp`
- **What:** During `merge()` / `apply_changes()`, independent changes (those with no
  dependency relationship in the change DAG) can be applied concurrently. Implementation:
  1. Topological sort the incoming changes by their dependency graph
  2. Identify independent "levels" — changes at the same level have no dependencies on each
     other
  3. Apply each level's changes in parallel using the thread pool
  4. Synchronize between levels
  ```
  auto levels = topological_levels(missing_changes);
  for (const auto& level : levels) {
      if (level.size() == 1) {
          apply_single_change(level[0]);  // fast path: no threading overhead
      } else {
          // Each change touches different objects or independent list regions
          pool.parallel_for(level.begin(), level.end(), [&](const Change& c) {
              apply_change_to_snapshot(c);
          });
          merge_snapshots();
      }
  }
  ```
- **Impact:** merge 2-4x faster for multi-actor workloads where changes are independent.
- **Risk:** MEDIUM-HIGH — must ensure CRDT commutativity is preserved. Changes modifying the
  same object must be serialized. Start conservatively: only parallelize changes to different
  ObjIds.
- **Depends on:** 11C.1 (thread pool).

### 11C.6 — Parallel column encoding during save
- **File:** `src/storage/columns/change_op_columns.hpp`
- **What:** The data collection pass (iterating ops to feed encoders) must be sequential, but
  the 14 encoder `finish()` calls and subsequent data extraction can be parallelized. More
  importantly, when saving a document with multiple changes, each change's column encoding
  is independent and can be done in parallel:
  ```
  auto& pool = ThreadPool::instance();
  auto encoded = std::vector<std::future<std::vector<RawColumn>>>{};
  for (const auto& change : state_->change_history) {
      encoded.push_back(pool.submit([&] {
          return encode_change_ops(change.operations, actor_table);
      }));
  }
  ```
- **Impact:** save with many changes: 2-4x faster (parallelizes across changes).
- **Depends on:** 11C.1 (thread pool).

### 11C.7 — `ThreadSafeDocument` wrapper
- **New file:** `include/automerge-cpp/thread_safe_document.hpp`
- **What:** A thread-safe wrapper around `Document` using `std::shared_mutex` for
  reader-writer locking. Allows multiple concurrent readers OR one exclusive writer.
- **Design:**
  ```
  class ThreadSafeDocument {
      Document doc_;
      mutable std::shared_mutex mtx_;
  public:
      // Read operations acquire shared lock
      auto get(const ObjId& obj, std::string_view key) const -> std::optional<Value>;
      auto text(const ObjId& obj) const -> std::string;
      auto keys(const ObjId& obj) const -> std::vector<std::string>;
      auto values(const ObjId& obj) const -> std::vector<Value>;
      auto length(const ObjId& obj) const -> std::size_t;
      auto save() const -> std::vector<std::byte>;
      auto get_heads() const -> std::vector<ChangeHash>;
      auto cursor(const ObjId& obj, std::size_t index) const -> std::optional<Cursor>;
      auto resolve_cursor(const ObjId& obj, const Cursor& c) const -> std::optional<std::size_t>;
      auto marks(const ObjId& obj) const -> std::vector<Mark>;
      // ... all const Document methods

      // Write operations acquire exclusive lock
      void transact(const std::function<void(Transaction&)>& fn);
      void merge(const Document& other);
      void apply_changes(const std::vector<Change>& changes);
      auto transact_with_patches(const std::function<void(Transaction&)>& fn) -> std::vector<Patch>;
      void receive_sync_message(SyncState& ss, const SyncMessage& msg);

      // Scoped access for complex operations
      template <typename F>
      auto read(F&& fn) const -> std::invoke_result_t<F, const Document&>;
      template <typename F>
      auto write(F&& fn) -> std::invoke_result_t<F, Document&>;

      // Access the underlying document (caller manages locking)
      auto unsafe_inner() -> Document&;
      auto unsafe_inner() const -> const Document&;
  };
  ```
- **Key features:**
  - Zero overhead for users who don't need thread safety (use `Document` directly)
  - `read()` and `write()` scoped accessors for complex multi-step operations
  - `unsafe_inner()` for advanced users who manage their own locking
  - All read methods share the lock (N readers concurrently)
  - Mutations are exclusive (1 writer, 0 readers)
- **Impact:** Enables safe multi-threaded access patterns without fork-and-merge overhead.
  Critical for server-side use cases (e.g., WebSocket server handling multiple clients).
- **Risk:** LOW — wrapper only, does not modify Document internals.
- **Depends on:** None (uses `Document` as-is).

### 11C.8 — Parallel sync protocol
- **File:** `src/document.cpp`
- **What:** In `generate_sync_message()`, the bloom filter construction and change discovery
  can be parallelized. When generating messages for multiple peers, each peer's sync can run
  concurrently:
  ```
  // Multi-peer parallel sync
  auto sync_all(std::vector<std::pair<SyncState&, Connection&>>& peers) {
      auto& pool = ThreadPool::instance();
      pool.parallel_for(peers.begin(), peers.end(), [&](auto& peer) {
          auto msg = doc.generate_sync_message(peer.first);
          if (msg) peer.second.send(*msg);
      });
  }
  ```
  Also parallelize `receive_sync_message()` for incoming messages from different peers
  (each peer's changes are independent until merge).
- **Impact:** Multi-peer sync N-x faster (one thread per peer).
- **Risk:** MEDIUM — `SyncState` is per-peer so no contention, but `Document` mutations from
  `receive_sync_message` must be serialized.
- **Depends on:** 11C.1 (thread pool), 11C.7 (ThreadSafeDocument for safe mutation).

---

## Phase 11D: Hardware Acceleration

### 11D.1 — ARM Crypto Extensions for SHA-256
- **File:** `src/crypto/sha256.hpp`
- **What:** Add compile-time-detected ARM SHA-256 hardware implementation using NEON
  intrinsics (`vsha256hq_u32`, etc.). Keep software fallback for x86/MSVC/FreeBSD. Optionally
  add SHA-NI (x86) path too.
- **Impact:** SHA-256 5-10x faster on M3 Max, benefits all hash-dependent paths.
- **Risk:** MEDIUM — platform-specific intrinsics, must maintain software fallback. Validated
  by existing SHA-256 test vectors.
- **Depends on:** 11B.4.

### 11D.2 — Cache-line alignment and SOA for ObjectState
- **File:** `src/doc_state.hpp`
- **What:** Add `alignas(64)` to `ObjectState`. Consider splitting `ListElement` into
  structure-of-arrays (separate vectors for `insert_id`, `visible`, `value`, `insert_after`)
  to improve cache locality for index-only operations.
- **Impact:** 20-40% faster list/cursor/text operations due to cache locality.
- **Risk:** MEDIUM — SOA touches many functions. Start with alignment only, measure, then
  consider SOA.
- **Depends on:** 11A.4.

---

## Phase 11E: Benchmark Expansion & Validation

### 11E.1 — Large document benchmarks
- **File:** `benchmarks/placeholder_benchmark.cpp`
- **What:** Add benchmarks for 10K/100K element lists, 10K-key maps, large merges (1000+
  changes), large save/load, large sync. These catch O(n^2) regressions invisible at small
  scale.

### 11E.2 — Multi-threaded benchmarks
- **File:** `benchmarks/placeholder_benchmark.cpp`
- **What:** Add benchmarks measuring multi-threaded throughput:
  - N threads doing concurrent reads on `ThreadSafeDocument`
  - Parallel save/load throughput vs. single-threaded
  - Parallel merge throughput (independent changes)
  - Thread pool overhead (submit + execute latency)
  - Object pool throughput (acquire/release cycles)

### 11E.3 — Before/after comparison
- **What:** Save v0.3.0 baseline as JSON, compare with v0.4.0 using
  `google-benchmark --benchmark_format=json`.

### 11E.4 — Regression verification
- **What:** All 274 existing tests pass after each sub-phase. Add debug-mode assertions to
  validate Fenwick tree against linear scan, hash cache against full rebuild, element index
  against linear search. Add thread safety tests for `ThreadSafeDocument`.

---

## Implementation Order

| Step | Item | Effort | Cumulative Impact |
|------|------|--------|-------------------|
| 1 | 11A.1 Remove op_log | 10 min | 10-15% mutations |
| 2 | 11A.2 unordered_map | 10 min | +15-30% lookups |
| 3 | 11B.1 Bloom stack array | 10 min | +5-10% sync |
| 4 | 11B.2 String copy elim | 20 min | +25% map_put |
| 5 | 11A.3 Hash cache | 30 min | +4-5x sync, +2x time travel |
| 6 | 11B.3 Serialization reserve | 15 min | +1.5x save |
| 7 | 11B.4 SHA-256 stack buf | 30 min | +10-20% hashing |
| 8 | 11B.5 Reserve throughout | 15 min | +2-5% cumulative |
| 9 | 11B.6 Object pool | 1 hr | +15-30% bulk mutations |
| 10 | 11A.4 Fenwick tree | 2 hr | +7-25x list/text |
| 11 | 11A.5 Element index | 1 hr | +1.5-2x merge |
| 12 | 11C.1 Thread pool | 1.5 hr | Foundation for parallelism |
| 13 | 11C.7 ThreadSafeDocument | 1 hr | Safe concurrent access |
| 14 | 11C.2+C.3 Parallel DEFLATE | 1 hr | +2-4x save/load large |
| 15 | 11C.4 Parallel SHA-256 | 30 min | +2-4x load large |
| 16 | 11C.6 Parallel column encode | 45 min | +2-4x save multi-change |
| 17 | 11C.5 Parallel merge | 2 hr | +2-4x merge multi-actor |
| 18 | 11C.8 Parallel sync | 1 hr | N-x multi-peer sync |
| 19 | 11D.1 ARM Crypto SHA-256 | 2 hr | +5-10x SHA-256 |
| 20 | 11D.2 Cache alignment/SOA | 1 hr | +20-40% list/cursor |
| 21 | 11E.1-4 Benchmarks + tests | 1.5 hr | Validation |

---

## New Files

| File | Description |
|------|-------------|
| `src/thread_pool.hpp` | Reusable thread pool (std::jthread, work queue, parallel_for) |
| `src/pool.hpp` | Typed object pool for Op, ListElement, MapEntry |
| `include/automerge-cpp/thread_safe_document.hpp` | ThreadSafeDocument (shared_mutex reader-writer) |

---

## Projected Results (v0.3.0 → v0.4.0)

| Benchmark | v0.3.0 | v0.4.0 (est.) | Speedup |
|-----------|--------|---------------|---------|
| map_put (single) | 953 K ops/s | ~1.4 M ops/s | 1.5x |
| map_put_batch/1000 | 3.44 M ops/s | ~5.5 M ops/s | 1.6x |
| map_get | 28.5 M ops/s | ~50 M ops/s | 1.8x |
| list_insert_append | 15.2 K ops/s | ~200 K ops/s | 13x |
| list_insert_front | 14.6 K ops/s | ~200 K ops/s | 14x |
| list_get (1000 elem) | 4.48 M ops/s | ~33 M ops/s | 7x |
| text_splice_bulk (100 chars) | 27 K chars/s | ~670 K chars/s | 25x |
| save (100 keys) | 10.4 K ops/s | ~18 K ops/s | 1.7x |
| load (100 keys) | 38.2 K ops/s | ~67 K ops/s | 1.7x |
| save_large (1000 items) | 1.38 KiB/s | ~5.5 KiB/s | 4x |
| merge (10+10 puts) | 248.7 K ops/s | ~750 K ops/s | 3x |
| sync_full_round_trip | 4.28 K ops/s | ~20 K ops/s | 4.7x |
| cursor_resolve | 6.04 M ops/s | ~40 M ops/s | 6.6x |
| **NEW: concurrent reads (8 threads)** | N/A | ~200 M ops/s | — |
| **NEW: parallel save (large doc)** | N/A | ~20 KiB/s | — |
| **NEW: parallel merge (4 actors)** | N/A | ~2 M ops/s | — |

---

## Verification

After each sub-phase:
```bash
cmake --build build && ctest --test-dir build --output-on-failure
```

After all phases, run release benchmarks:
```bash
cmake -B build-release -DCMAKE_BUILD_TYPE=Release \
    -DAUTOMERGE_CPP_BUILD_BENCHMARKS=ON \
    -DAUTOMERGE_CPP_BUILD_TESTS=ON
cmake --build build-release
./build-release/benchmarks/automerge_cpp_benchmarks
```

Compare against v0.3.0 baseline numbers documented in `docs/benchmark-results.md`.

---

## Critical Files

| File | Optimizations |
|------|--------------|
| `src/doc_state.hpp` | 11A.1-5, 11B.5, 11D.2 (central target) |
| `src/transaction.cpp` | 11A.1, 11B.2, 11B.6 |
| `src/document.cpp` | 11A.1, 11B.3, 11B.5, 11C.5, 11C.6 |
| `src/crypto/sha256.hpp` | 11B.4, 11D.1 |
| `src/storage/change_chunk.hpp` | 11B.3, 11C.2, 11C.3 |
| `src/sync/bloom_filter.hpp` | 11B.1 |
| `src/storage/columns/raw_column.hpp` | 11B.3 |
| `src/thread_pool.hpp` | 11C.1 (NEW) |
| `src/pool.hpp` | 11B.6 (NEW) |
| `include/automerge-cpp/thread_safe_document.hpp` | 11C.7 (NEW) |
| `benchmarks/placeholder_benchmark.cpp` | 11E.1-2 |
| `docs/plans/roadmap.md` | Update with Phase 11 |
