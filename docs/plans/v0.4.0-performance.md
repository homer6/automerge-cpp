# v0.4.0 Performance Release Plan — automerge-cpp

## Context

automerge-cpp v0.3.0 is functionally complete (274 tests, 10 phases done) but has significant
performance bottlenecks. The biggest issue is `visible_index_to_real()` — an O(n) linear scan
called on every list/text operation. Other bottlenecks include quadratic hash index rebuilds in
the sync path, duplicate op storage, unnecessary heap allocations, and zero parallelism.

This plan targets 2-25x improvements across all benchmark categories through:
1. **Algorithmic fixes** (Fenwick tree, hash cache, element index)
2. **Allocation reduction** (stack buffers, object pool, reserve, string dedup)
3. **Transparent internal parallelism** (thread pool powering save/load/hash/sync)
4. **Hardware acceleration** (ARM SHA-256 intrinsics, cache-line alignment)

### Design Principle: Transparent Parallelism

The `Document` API stays **exactly the same**. Same inputs → same deterministic outputs.
The thread pool is an **implementation detail** — users never see threads, locks, or
concurrency. This is the correct design because:

- **Fork-per-thread breaks semantics.** Forking creates a new ActorId, which changes OpIds,
  change hashes, and conflict resolution order. `transact(put key=1); transact(put key=2)` on
  one doc is NOT equivalent to fork→put→put→merge. The internal state differs.
- **Internal parallelism is invisible.** `save()` returns the same bytes whether it used 1
  thread or 30. `load()` produces the same document. `merge()` produces the same state.
- **No API leakage.** Users don't partition work, manage thread counts, or handle merges.

`ThreadSafeDocument` is offered separately as an opt-in wrapper for concurrent access patterns
(multiple threads reading/writing the same doc). It is orthogonal to internal parallelism.

**Target platform for local testing:** Apple M3 Max (16 cores) / 30-core server, macOS/Linux,
Apple Clang / GCC, Release build.

---

## Phase 11A: Core Data Structure Optimizations

### 11A.1 — Remove dead `op_log` field
- **Files:** `src/doc_state.hpp`, `src/transaction.cpp`, `src/document.cpp`
- **What:** Remove `std::vector<Op> op_log` from `DocState` and all `push_back` calls. It is
  never read — `change_history` stores all ops inside `Change.operations`.
- **Impact:** ~10-15% faster mutations, ~40% memory reduction for op storage.
- **Risk:** LOW — dead code removal.

### 11A.2 — Replace `std::map<ObjId, ObjectState>` with `std::unordered_map`
- **File:** `src/doc_state.hpp`
- **What:** Change `objects` from `std::map` to `std::unordered_map`. `std::hash<ObjId>`
  already exists in `types.hpp`.
- **Impact:** O(log n) → O(1) object lookup. ~15% faster map_put, ~30% faster map_get.
- **Risk:** LOW — hash already implemented, no code depends on iteration order.

### 11A.3 — Cache the change hash index
- **File:** `src/doc_state.hpp`
- **What:** Add a cached `std::unordered_map<ChangeHash, std::size_t>` to `DocState`. Rebuild
  incrementally (changes are append-only). Eliminate the O(n) `change_hash_index()` rebuild on
  every sync/time-travel call. Fix `has_change_hash()` which currently rebuilds the full index
  per call (making `get_missing_deps` quadratic).
- **API:** Replace `change_hash_index()` with `ensure_hash_index()` that lazily builds and
  caches. All callers use the cached version. Invalidate by tracking
  `cached_hash_index_size_` — if `change_history.size()` has grown, append new entries.
- **Impact:** sync_full_round_trip 4-5x faster, get_at 2x faster.
- **Risk:** LOW — cache invalidation is trivial (append-only data).

### 11A.4 — Fenwick tree for `visible_index_to_real`
- **File:** `src/doc_state.hpp`
- **What:** Add a `FenwickTree` (Binary Indexed Tree) to `ObjectState` for O(log n)
  visible-index-to-real-index conversion via `find_kth` (binary lifting). Maintain on insert
  (+1) and delete (flip visibility). Replace the O(n) linear scan at line 193.
- **Design:**
  ```
  class FenwickTree {
      std::vector<int> tree_;  // tree_[i] = partial sum of visible elements
  public:
      void insert(std::size_t pos, int val);   // insert element, shift tree
      void update(std::size_t pos, int delta); // toggle visibility (+1/-1)
      auto prefix_sum(std::size_t pos) const -> int;
      auto find_kth(int k) const -> std::size_t; // O(log n) binary lifting
      auto total() const -> int;
  };
  ```
- **Integration:** `ObjectState` gains `FenwickTree visibility_tree`. Updated by:
  - `list_insert()`: insert +1 at real_idx
  - `list_delete()`: update -1 at real_idx
  - `apply_op(insert/splice_text)`: insert +1 at rga_pos
  - `apply_op(del)`: update -1 at element position
- **Debug validation:** In debug builds, assert `find_kth` result matches linear scan.
- **Impact:** list/text operations 7-25x faster. This is the single highest-impact optimization.
- **Risk:** LOW-MEDIUM — well-understood data structure, validated against linear scan.

### 11A.5 — OpId-to-position index for RGA merge
- **File:** `src/doc_state.hpp`
- **What:** Add `std::unordered_map<OpId, std::size_t> element_index` to `ObjectState` mapping
  `insert_id → real_index`. Replace linear search in `find_rga_position` (line 380) with O(1)
  hash lookup. Maintain on insert (shift all indices >= insertion point).
- **Impact:** merge 1.5-2x faster, critical for large document merges.
- **Risk:** MEDIUM — index maintenance on vector insert requires careful handling.
- **Depends on:** 11A.4 (both modify list element management).

---

## Phase 11B: Memory & Allocation Optimizations

### 11B.1 — Bloom filter stack-allocated probe array
- **File:** `src/sync/bloom_filter.hpp`
- **What:** Change `get_probes()` return from `std::vector<uint32_t>` to
  `std::array<uint32_t, 7>`. Eliminates heap allocation per bloom filter check.
- **Impact:** 5-10% faster sync operations.
- **Risk:** LOW.

### 11B.2 — String copy elimination in Transaction
- **File:** `src/transaction.cpp`
- **What:** In `put()`, `put_object()`, `delete_key()`, `increment()`, `mark()` — create
  `std::string` from key once and move it through the call chain, instead of creating 3 copies
  per operation. Change `map_pred()`, `map_put()`, `map_delete()`, `counter_increment()` to
  accept `const std::string&` or take by move where the string is consumed.
- **Impact:** ~25% faster map_put (saves 2 heap allocations per put).
- **Risk:** LOW — pure refactoring.

### 11B.3 — Serialization buffer pre-sizing
- **Files:** `src/document.cpp`, `src/storage/change_chunk.hpp`, `src/storage/columns/raw_column.hpp`
- **What:** Add `reserve()` calls with size heuristics before building output buffers.
  E.g., `body.reserve(128 + change_history.size() * 256)` in `save()`.
  In `serialize_change_body()`: `body.reserve(64 + ops.size() * 32)`.
  In `write_raw_columns()`: `output.reserve(output.size() + total_column_bytes)`.
- **Impact:** save 1.5-1.7x faster, save_large 1.7x faster.
- **Risk:** LOW.

### 11B.4 — SHA-256 stack-allocated padding buffer
- **File:** `src/crypto/sha256.hpp`
- **What:** Replace heap-allocated `std::vector<std::byte>` padding with a stack buffer.
  Use `std::array<std::byte, 256>` for inputs up to 247 bytes (covers typical change hashing).
  Fall back to heap for larger inputs.
- **Impact:** 10-20% faster SHA-256, benefits all save/load/commit/sync paths.
- **Risk:** LOW — output verified by existing 7 NIST test vectors.

### 11B.5 — Reserve-based pre-allocation throughout
- **Files:** `src/document.cpp`, `src/doc_state.hpp`
- **What:** Add `reserve()` in `ops_to_patches()`, `build_actor_table()`, `all_change_hashes()`,
  `get_changes_since()`, `get_missing_deps()`, `get_changes_by_hash()`, `changes_visible_at()`,
  and other vector-building functions where size is known or estimable.
- **Impact:** 2-5% cumulative improvement.
- **Risk:** LOW.

### 11B.6 — Object pool for Op and ListElement
- **New file:** `src/pool.hpp`
- **What:** Implement a typed arena/object pool (`ObjectPool<T>`) that pre-allocates blocks of
  frequently created/destroyed objects. Use it for `Op` in the transaction hot path.
- **Design:**
  ```
  template <typename T, std::size_t BlockSize = 256>
  class ObjectPool {
      std::vector<std::unique_ptr<std::array<T, BlockSize>>> blocks_;
      std::vector<T*> free_list_;
  public:
      auto acquire() -> T*;        // O(1) amortized
      void release(T* obj);        // O(1)
      void reserve(std::size_t n); // pre-allocate n slots
  };
  ```
- **Usage in `Transaction`:** Instead of `pending_ops_.push_back(std::move(op))` allocating
  from the global heap on every operation, acquire from the pool. On `commit()`, ops move
  into the `Change` and pool slots are released.
- **Impact:** Eliminates per-operation heap allocation/deallocation churn. 15-30% faster for
  bulk transaction workloads (map_put_batch, text_splice_bulk).
- **Risk:** LOW-MEDIUM — pool is simple, integration touches hot paths. Pool owned by `DocState`.

---

## Phase 11C: Thread Pool & Transparent Internal Parallelism

### Architecture

All parallelism is **internal to the library**. The `Document` API is unchanged. A thread pool
is used as an implementation detail to accelerate save, load, hashing, and sync. The pool is a
process-global singleton sized to `std::thread::hardware_concurrency()` (16-30 cores).

Parallelism is only engaged when the workload justifies it (e.g., >32 items to process).
Below that threshold, the sequential path runs with zero overhead.

### 11C.1 — Thread pool (`src/thread_pool.hpp`)
- **New file:** `src/thread_pool.hpp`
- **What:** Implement a lightweight, header-only thread pool using C++23 primitives.
- **Design:**
  ```
  class ThreadPool {
      std::vector<std::jthread> workers_;
      std::queue<std::move_only_function<void()>> tasks_;
      std::mutex mutex_;
      std::condition_variable_any cv_;
      std::stop_source stop_;

  public:
      explicit ThreadPool(std::size_t n = std::thread::hardware_concurrency());
      ~ThreadPool();  // signals stop, joins all workers

      // Submit a callable, get a future for its result
      template <typename F>
      auto submit(F&& f) -> std::future<std::invoke_result_t<F>>;

      // Data-parallel: partition [begin, end) across threads, call fn(item) for each
      template <typename Iter, typename F>
      void parallel_for_each(Iter begin, Iter end, F&& fn);

      // Index-parallel: call fn(i) for i in [0, count), partitioned across threads
      template <typename F>
      void parallel_for(std::size_t count, F&& fn);

      // Parallel transform: result[i] = fn(i) for i in [0, count)
      template <typename T, typename F>
      auto parallel_map(std::size_t count, F&& fn) -> std::vector<T>;

      auto thread_count() const -> std::size_t;

      // Global singleton (created on first use, destroyed at exit)
      static auto instance() -> ThreadPool&;
  };
  ```
- **Key features:**
  - `std::jthread` with cooperative `std::stop_token` for clean shutdown
  - `std::move_only_function` (C++23) for type-erased task storage (no `std::function` copy overhead)
  - Simple shared queue with mutex (work-stealing unnecessary at this scale)
  - `parallel_for` / `parallel_map` / `parallel_for_each` helpers for data-parallel loops
  - Minimum work threshold: only dispatch to threads if work count > 2 × thread_count
  - Default: `std::hardware_concurrency()` threads (30 on target server)
- **Impact:** Foundation for all parallel work below.
- **Risk:** LOW — `std::jthread` handles lifetime; well-understood pattern.

### 11C.2 — Parallel change serialization during save
- **File:** `src/document.cpp`
- **What:** In `Document::save()`, serialize each change body in parallel. The current code:
  ```
  for (const auto& change : state_->change_history) {
      auto change_body = storage::serialize_change_body(change, actor_table);
      // ... append to output
  }
  ```
  Becomes:
  ```
  auto& pool = ThreadPool::instance();
  auto bodies = pool.parallel_map<std::vector<std::byte>>(
      state_->change_history.size(),
      [&](std::size_t i) {
          return storage::serialize_change_body(state_->change_history[i], actor_table);
      }
  );
  // Sequential assembly of output from bodies
  ```
  Each `serialize_change_body()` is fully independent — it reads the `Change` and
  `actor_table` (both const) and produces a byte vector. No shared mutable state.
- **Nested parallelism:** Within each `serialize_change_body()`, column compression is also
  parallelizable (11C.3). On a 30-core machine with 100 changes, we get 30 changes
  serializing simultaneously, each potentially compressing columns in parallel.
- **Impact:** save with N changes: up to min(N, 30)x faster. For 1000-change documents
  on a 30-core machine: ~25-30x faster serialization.
- **Risk:** LOW — each call is pure (reads const data, writes to independent output).

### 11C.3 — Parallel DEFLATE compression during save
- **File:** `src/storage/change_chunk.hpp`
- **What:** In `serialize_change_body()`, after column encoding, compress columns >256 bytes
  in parallel:
  ```
  auto& pool = ThreadPool::instance();
  auto futures = std::vector<std::pair<std::size_t, std::future<std::optional<std::vector<std::byte>>>>>{};
  for (std::size_t i = 0; i < columns.size(); ++i) {
      if (columns[i].data.size() > deflate_threshold) {
          futures.emplace_back(i, pool.submit([&col = columns[i]] {
              return deflate_compress(col.data);
          }));
      }
  }
  // Collect results and replace column data
  ```
  Each `z_stream` is independent. zlib is thread-safe for independent streams.
- **Impact:** Per-change serialization: up to 14 columns compressed in parallel.
  Combined with 11C.2 (parallel across changes): multiplicative speedup.
- **Risk:** LOW — each compression is independent.

### 11C.4 — Parallel DEFLATE decompression during load
- **File:** `src/storage/change_chunk.hpp`
- **What:** In `parse_change_chunk()`, decompress all compressed columns in parallel.
  Same pattern as 11C.3 but for `deflate_decompress()`.
- **Impact:** load 2-4x faster per change chunk for compressed documents.
- **Risk:** LOW — same pattern as 11C.3.

### 11C.5 — Parallel change chunk parsing during load
- **File:** `src/document.cpp`
- **What:** In `parse_v2()`, after reading the chunk headers (which gives us each change
  chunk's byte range), parse all change chunks in parallel:
  ```
  // Sequential: read chunk offsets
  auto chunk_spans = std::vector<std::span<const std::byte>>{};
  for (...) { chunk_spans.push_back(body.subspan(pos, len)); pos += len; }

  // Parallel: parse each chunk (decompress + decode columns)
  auto& pool = ThreadPool::instance();
  auto changes = pool.parallel_map<std::optional<Change>>(
      chunk_spans.size(),
      [&](std::size_t i) {
          return storage::parse_change_chunk(chunk_spans[i], actor_table);
      }
  );

  // Sequential: apply changes (correctness requires ordering)
  for (auto& change : changes) {
      if (!change) return std::nullopt;
      // ... apply ops
  }
  ```
  Each `parse_change_chunk()` is independent — reads from its own byte range, uses the
  shared `actor_table` (const), writes to its own `Change` output.
- **Impact:** Load with N changes: up to min(N, 30)x faster for the parsing phase.
  The apply phase remains sequential (CRDT correctness requires causal ordering).
- **Risk:** LOW — each parse is pure.

### 11C.6 — Parallel SHA-256 hash computation
- **File:** `src/doc_state.hpp`
- **What:** When computing hashes for multiple changes (cache rebuild, `all_change_hashes()`,
  `get_changes_since()`), hash in parallel:
  ```
  auto& pool = ThreadPool::instance();
  auto hashes = pool.parallel_map<ChangeHash>(
      change_history.size(),
      [&](std::size_t i) { return compute_change_hash(change_history[i]); }
  );
  ```
  Each SHA-256 computation is completely independent — reads a `Change` (const), writes a
  32-byte hash.
- **Impact:** For 1000 changes on 30 cores: ~25-30x faster hash computation.
  Benefits: hash cache rebuild on load, `all_change_hashes()`, `get_changes_since()`.
- **Risk:** LOW — each hash is pure.

### 11C.7 — Parallel column encoding during save
- **File:** `src/storage/columns/change_op_columns.hpp`
- **What:** In `encode_change_ops()`, the op iteration loop (feeding encoders) must be
  sequential because encoder state depends on previous ops. But the 14 `finish()` + `take()`
  calls are independent and can be parallelized. More importantly, when combined with 11C.2,
  each change's ENTIRE column encoding is already running on its own thread.
  Additionally, the 14 encoders' `take()` calls can run in parallel within a single change:
  ```
  // After the sequential op loop, finish all encoders in parallel
  auto& pool = ThreadPool::instance();
  pool.parallel_for(14, [&](std::size_t i) {
      // finish() and take() for encoder i
  });
  ```
- **Impact:** Marginal within a single change (finish is fast), but significant when combined
  with 11C.2 for many-change documents.
- **Risk:** LOW.

### 11C.8 — Parallel bloom filter construction
- **File:** `src/sync/bloom_filter.hpp`
- **What:** When building a bloom filter from many hashes (in `generate_sync_message()`),
  compute probe positions in parallel. Since `set_bit()` uses byte-level OR, use
  `std::atomic<uint8_t>` for the bit array during parallel construction:
  ```
  auto& pool = ThreadPool::instance();
  pool.parallel_for_each(hashes.begin(), hashes.end(), [&](const ChangeHash& h) {
      auto probes = get_probes(h);
      for (auto p : probes) {
          atomic_bits_[p >> 3].fetch_or(1u << (p & 7), std::memory_order_relaxed);
      }
  });
  ```
  Copy atomic array back to regular array after construction.
- **Impact:** For documents with 1000+ changes, bloom construction is 10-25x faster.
- **Risk:** LOW-MEDIUM — atomic ops are well-understood; relaxed ordering is sufficient for OR.

### 11C.9 — ThreadSafeDocument wrapper (opt-in)
- **New file:** `include/automerge-cpp/thread_safe_document.hpp`
- **What:** A thread-safe wrapper around `Document` using `std::shared_mutex` for
  reader-writer locking. Allows N concurrent readers OR 1 exclusive writer. This is
  **orthogonal** to internal parallelism — it's for users who need multiple threads
  accessing the same document (e.g., WebSocket server handling multiple clients).
- **Design:**
  ```
  class ThreadSafeDocument {
      Document doc_;
      mutable std::shared_mutex mtx_;
  public:
      // Read operations acquire shared lock (N readers concurrently)
      auto get(...) const -> ...;
      auto text(...) const -> ...;
      auto save() const -> ...;
      // ... all const Document methods

      // Write operations acquire exclusive lock (1 writer, 0 readers)
      void transact(...);
      void merge(...);
      void apply_changes(...);
      void receive_sync_message(...);

      // Scoped access for complex multi-step operations
      template <typename F> auto read(F&& fn) const -> ...;
      template <typename F> auto write(F&& fn) -> ...;

      // Escape hatch (caller manages locking)
      auto unsafe_inner() -> Document&;
  };
  ```
- **Key features:**
  - Zero overhead for users who don't need it (use `Document` directly)
  - `read()` / `write()` scoped accessors for complex multi-step operations
  - All read methods share the lock
  - Does NOT change Document internals — pure wrapper
- **Impact:** Enables safe multi-threaded access patterns.
- **Risk:** LOW — wrapper only.

---

## Phase 11D: Hardware Acceleration

### 11D.1 — ARM Crypto Extensions for SHA-256
- **File:** `src/crypto/sha256.hpp`
- **What:** Add compile-time-detected ARM SHA-256 hardware implementation using NEON
  intrinsics (`vsha256hq_u32`, `vsha256h2q_u32`, `vsha256su0q_u32`, `vsha256su1q_u32`).
  Keep software fallback for x86/MSVC/FreeBSD. Optionally add x86 SHA-NI path.
- **Detection:** `#if defined(__ARM_FEATURE_SHA2) || (defined(__aarch64__) && defined(__APPLE__))`
- **Impact:** SHA-256 5-10x faster on Apple Silicon / ARMv8.2+.
  Combined with 11C.6 (parallel hashing): multiplicative — 30 cores x 5-10x per core.
- **Risk:** MEDIUM — platform-specific intrinsics, must maintain software fallback.
  Validated by existing 7 SHA-256 NIST test vectors.
- **Depends on:** 11B.4.

### 11D.2 — Cache-line alignment for ObjectState
- **File:** `src/doc_state.hpp`
- **What:** Add `alignas(64)` to `ObjectState` to prevent false sharing when multiple
  objects are accessed from different threads during parallel operations. Add `alignas(64)`
  to `FenwickTree` internal vector as well.
- **Impact:** 10-20% faster when objects are accessed in parallel (e.g., during parallel
  change application or parallel bloom filter probes that touch different objects).
- **Risk:** LOW — alignment only, no structural changes.
- **Depends on:** 11A.4.

---

## Phase 11E: Benchmark Expansion & Validation

### 11E.1 — Large document benchmarks
- **File:** `benchmarks/placeholder_benchmark.cpp`
- **What:** Add benchmarks for:
  - 10K/100K element lists (catches O(n^2) in Fenwick tree and element index)
  - 10K-key maps
  - Large merges (100+ changes from multiple actors)
  - Large save/load (1000-change documents)
  - Large sync (documents with significant divergence)
  - Parallel save throughput (how well does it scale with core count?)
  - Parallel load throughput

### 11E.2 — Multi-threaded benchmarks
- **File:** `benchmarks/placeholder_benchmark.cpp`
- **What:** Add benchmarks measuring:
  - N threads doing concurrent reads on `ThreadSafeDocument`
  - Parallel save vs sequential save (varying change count: 10, 100, 1000)
  - Parallel load vs sequential load
  - Thread pool overhead (submit + execute latency for trivial tasks)
  - Object pool throughput (acquire/release cycles)

### 11E.3 — Before/after comparison
- **What:** Save v0.3.0 baseline as JSON, compare with v0.4.0 using
  `google-benchmark --benchmark_format=json`.

### 11E.4 — Regression verification
- **What:** All 274 existing tests pass after each sub-phase. Add debug-mode assertions to
  validate Fenwick tree against linear scan, hash cache against full rebuild, element index
  against linear search. Add thread safety tests for `ThreadSafeDocument`.

---

## Implementation Order

| Step | Item | Effort | Cumulative Impact |
|------|------|--------|-------------------|
| 1 | 11A.1 Remove op_log | 10 min | 10-15% mutations |
| 2 | 11A.2 unordered_map | 10 min | +15-30% lookups |
| 3 | 11B.1 Bloom stack array | 10 min | +5-10% sync |
| 4 | 11B.2 String copy elim | 20 min | +25% map_put |
| 5 | 11A.3 Hash cache | 30 min | +4-5x sync, +2x time travel |
| 6 | 11B.3 Serialization reserve | 15 min | +1.5x save |
| 7 | 11B.4 SHA-256 stack buf | 30 min | +10-20% hashing |
| 8 | 11B.5 Reserve throughout | 15 min | +2-5% cumulative |
| 9 | 11B.6 Object pool | 1 hr | +15-30% bulk mutations |
| 10 | 11A.4 Fenwick tree | 2 hr | +7-25x list/text |
| 11 | 11A.5 Element index | 1 hr | +1.5-2x merge |
| 12 | 11C.1 Thread pool | 1.5 hr | Foundation for parallelism |
| 13 | 11C.2 Parallel change serialization | 45 min | Up to 30x save (many changes) |
| 14 | 11C.3 Parallel column compression | 30 min | +2-4x per-change save |
| 15 | 11C.4 Parallel column decompression | 30 min | +2-4x per-change load |
| 16 | 11C.5 Parallel change chunk parsing | 45 min | Up to 30x load parsing |
| 17 | 11C.6 Parallel SHA-256 hashing | 30 min | Up to 30x hash computation |
| 18 | 11C.7 Parallel column encoding | 30 min | Additional save speedup |
| 19 | 11C.8 Parallel bloom construction | 30 min | +10-25x bloom for large docs |
| 20 | 11C.9 ThreadSafeDocument | 1 hr | Safe concurrent access |
| 21 | 11D.1 ARM Crypto SHA-256 | 2 hr | +5-10x per-core SHA-256 |
| 22 | 11D.2 Cache alignment | 30 min | +10-20% parallel access |
| 23 | 11E.1-4 Benchmarks + tests | 1.5 hr | Validation |

---

## Parallelism Map: What Runs on 30 Cores

| Operation | Sequential Part | Parallel Part | Max Speedup (30 cores) |
|-----------|----------------|---------------|----------------------|
| `save()` | Header assembly | Change body serialization (11C.2) | 25-30x |
| `save()` (per change) | Op loop for encoders | Column compression (11C.3) | 14x |
| `load()` | Header parse | Change chunk parsing (11C.5) | 25-30x |
| `load()` (per chunk) | Column decode | Column decompression (11C.4) | 14x |
| `load()` | — | Hash cache rebuild (11C.6) | 25-30x |
| `all_change_hashes()` | — | SHA-256 per change (11C.6) | 25-30x |
| `generate_sync_message()` | BFS traversal | Bloom filter build (11C.8) | 10-25x |
| SHA-256 (per call) | — | ARM intrinsics (11D.1) | 5-10x per core |

**Combined example — save a 1000-change document on 30 cores:**
- Sequential: serialize 1000 changes x (column encode + compress) = ~1000 x 1ms = 1s
- Parallel: 1000 changes / 30 cores = ~34 batches x 1ms = 34ms → **~30x speedup**

**Combined example — load a 1000-change compressed document on 30 cores:**
- Parallel parse: 1000 chunks / 30 cores = ~34 batches → **~30x faster parsing**
- Sequential apply: unchanged (CRDT correctness)
- Parallel hash cache: 1000 hashes / 30 cores → **~30x faster hashing**

---

## New Files

| File | Description |
|------|-------------|
| `src/thread_pool.hpp` | Thread pool (std::jthread, work queue, parallel_for, parallel_map) |
| `src/pool.hpp` | Typed object pool for Op, ListElement |
| `include/automerge-cpp/thread_safe_document.hpp` | ThreadSafeDocument (shared_mutex wrapper) |

---

## Projected Results (v0.3.0 → v0.4.0)

| Benchmark | v0.3.0 | v0.4.0 (est.) | Speedup |
|-----------|--------|---------------|---------|
| map_put (single) | 953 K ops/s | ~1.4 M ops/s | 1.5x |
| map_put_batch/1000 | 3.44 M ops/s | ~5.5 M ops/s | 1.6x |
| map_get | 28.5 M ops/s | ~50 M ops/s | 1.8x |
| list_insert_append | 15.2 K ops/s | ~200 K ops/s | 13x |
| list_insert_front | 14.6 K ops/s | ~200 K ops/s | 14x |
| list_get (1000 elem) | 4.48 M ops/s | ~33 M ops/s | 7x |
| text_splice_bulk (100 chars) | 27 K chars/s | ~670 K chars/s | 25x |
| save (100 keys, 1 change) | 10.4 K ops/s | ~18 K ops/s | 1.7x |
| save (1000 changes) | — | — | ~25-30x |
| load (100 keys, 1 change) | 38.2 K ops/s | ~67 K ops/s | 1.7x |
| load (1000 changes) | — | — | ~20-25x |
| save_large (1000 items) | 1.38 KiB/s | ~35 KiB/s | 25x |
| merge (10+10 puts) | 248.7 K ops/s | ~500 K ops/s | 2x |
| sync_full_round_trip | 4.28 K ops/s | ~20 K ops/s | 4.7x |
| cursor_resolve | 6.04 M ops/s | ~40 M ops/s | 6.6x |
| **NEW: parallel save (1000 changes, 30 cores)** | N/A | benchmark | — |
| **NEW: parallel load (1000 changes, 30 cores)** | N/A | benchmark | — |
| **NEW: concurrent reads (ThreadSafeDocument, 8 threads)** | N/A | benchmark | — |

---

## Verification

After each sub-phase:
```bash
cmake --build build && ctest --test-dir build --output-on-failure
```

After all phases, run release benchmarks:
```bash
cmake -B build-release -DCMAKE_BUILD_TYPE=Release \
    -DAUTOMERGE_CPP_BUILD_BENCHMARKS=ON \
    -DAUTOMERGE_CPP_BUILD_TESTS=ON
cmake --build build-release
./build-release/benchmarks/automerge_cpp_benchmarks
```

Compare against v0.3.0 baseline numbers documented in `docs/benchmark-results.md`.

---

## Critical Files

| File | Optimizations |
|------|--------------|
| `src/doc_state.hpp` | 11A.1-5, 11B.5, 11D.2 (central target) |
| `src/transaction.cpp` | 11A.1, 11B.2, 11B.6 |
| `src/document.cpp` | 11A.1, 11B.3, 11B.5, 11C.2, 11C.5 |
| `src/crypto/sha256.hpp` | 11B.4, 11D.1 |
| `src/storage/change_chunk.hpp` | 11B.3, 11C.3, 11C.4 |
| `src/storage/columns/change_op_columns.hpp` | 11C.7 |
| `src/sync/bloom_filter.hpp` | 11B.1, 11C.8 |
| `src/storage/columns/raw_column.hpp` | 11B.3 |
| `src/thread_pool.hpp` | 11C.1 (NEW) |
| `src/pool.hpp` | 11B.6 (NEW) |
| `include/automerge-cpp/thread_safe_document.hpp` | 11C.9 (NEW) |
| `benchmarks/placeholder_benchmark.cpp` | 11E.1-2 |
| `docs/plans/roadmap.md` | Update with Phase 11 |
