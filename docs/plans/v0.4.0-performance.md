# v0.4.0 Performance Release Plan — automerge-cpp

## Context

automerge-cpp v0.3.0 is functionally complete (274 tests, 10 phases done) but has significant
performance bottlenecks. The biggest issue is `visible_index_to_real()` — an O(n) linear scan
called on every list/text operation. Other bottlenecks include quadratic hash index rebuilds in
the sync path, duplicate op storage, unnecessary heap allocations, and zero parallelism.

This plan targets 2-25x improvements across all benchmark categories through:
1. **Algorithmic fixes** (Fenwick tree, hash cache, element index)
2. **Allocation reduction** (stack buffers, object pool, reserve, string dedup)
3. **Transparent internal parallelism** (Document-owned thread pool for save/load/hash/sync)
4. **Hardware acceleration** (ARM SHA-256 intrinsics, x86 SHA-NI, cache-line alignment)

### Design Principle: Monoid-Powered Parallelism

Per the project's style guide (Ben Deane's *Composable C++*), section 2.3:

> "If the binary operation is associative with an identity, it is a monoid —
> and can be parallelized."

CRDT merge **is** a monoid:
- **Binary op:** `merge(a, b)`
- **Identity:** empty `Document{}`
- **Associative:** `merge(merge(a, b), c) == merge(a, merge(b, c))`
- **Commutative:** `merge(a, b) == merge(b, a)`
- **Idempotent:** `merge(a, a) == a`

### Threading Model

**Zero external dependencies.** No Taskflow, no TBB, no execution policies. The thread pool
is built from `std::jthread` and `std::atomic` — standard C++20 only.

**Document owns its thread pool.** The constructor accepts an optional thread count:

```cpp
// Use all cores (default)
auto doc = Document{};

// Explicit thread count
auto doc = Document{8};

// Single-threaded (no pool, zero overhead)
auto doc = Document{1};
```

**Document is thread-safe by default** using `std::shared_mutex` internally:
- Read methods (`get`, `text`, `keys`, `values`, `length`, `save`, etc.) acquire a **shared
  lock** — N readers run concurrently.
- Write methods (`transact`, `merge`, `apply_changes`, `receive_sync_message`) acquire an
  **exclusive lock**.
- **1 document with 120 threads works.** 1000 documents with 30 threads works.

**Internal parallelism is invisible.** `save()` returns the same bytes whether it used 1
core or 30. `load()` produces the same document. All parallelism is inside the Document
implementation — the API is unchanged except for the constructor parameter.

**Target platforms for local testing:**
- **macOS:** Apple M3 Max (16 cores), Apple Clang, Release build
- **Linux:** Intel Xeon Platinum 8358 (30 cores @ 2.6 GHz, SHA-NI, AVX-512), GCC 13.3, Release build

---

## Phase 11A: Core Data Structure Optimizations

### 11A.1 — Remove dead `op_log` field
- **Files:** `src/doc_state.hpp`, `src/transaction.cpp`, `src/document.cpp`
- **What:** Remove `std::vector<Op> op_log` from `DocState` and all `push_back` calls. It is
  never read — `change_history` stores all ops inside `Change.operations`.
- **Impact:** ~10-15% faster mutations, ~40% memory reduction for op storage.
- **Risk:** LOW — dead code removal.

### 11A.2 — Replace `std::map<ObjId, ObjectState>` with `std::unordered_map`
- **File:** `src/doc_state.hpp`
- **What:** Change `objects` from `std::map` to `std::unordered_map`. `std::hash<ObjId>`
  already exists in `types.hpp`.
- **Impact:** O(log n) → O(1) object lookup. ~15% faster map_put, ~30% faster map_get.
- **Risk:** LOW — hash already implemented, no code depends on iteration order.

### 11A.3 — Cache the change hash index
- **File:** `src/doc_state.hpp`
- **What:** Add a cached `std::unordered_map<ChangeHash, std::size_t>` to `DocState`. Rebuild
  incrementally (changes are append-only). Eliminate the O(n) `change_hash_index()` rebuild on
  every sync/time-travel call. Fix `has_change_hash()` which currently rebuilds the full index
  per call (making `get_missing_deps` quadratic).
- **API:** Replace `change_hash_index()` with `ensure_hash_index()` that lazily builds and
  caches. All callers use the cached version. Invalidate by tracking
  `cached_hash_index_size_` — if `change_history.size()` has grown, append new entries.
- **Impact:** sync_full_round_trip 4-5x faster, get_at 2x faster.
- **Risk:** LOW — cache invalidation is trivial (append-only data).

### 11A.3b — Cache the actor table
- **Files:** `src/doc_state.hpp`, `src/document.cpp`
- **What:** Cache the actor table (`std::vector<ActorId>`) in `DocState`. Currently
  `build_actor_table()` is called on every `save()` and iterates all ops in all changes to
  build an `unordered_set<ActorId>`, then converts to a sorted vector. For single-actor
  documents, this is pure waste. Cache the table and invalidate only when actors change
  (merge, load, apply_changes from a different actor).
- **Discovery:** Linux profiling showed `Hashtable::_M_insert_unique` (ActorId) at **15.58%**
  of `save_large` — second highest cost after `encode_change_ops`. This was hidden on macOS
  by the higher relative cost of software SHA-256.
- **Impact:** save_large 1.2-1.5x faster. save 1.1x faster.
- **Risk:** LOW — cache invalidation only on actor set change (append-only).

### 11A.4 — Fenwick tree for `visible_index_to_real`
- **File:** `src/doc_state.hpp`
- **What:** Add a `FenwickTree` (Binary Indexed Tree) to `ObjectState` for O(log n)
  visible-index-to-real-index conversion via `find_kth` (binary lifting). Maintain on insert
  (+1) and delete (flip visibility). Replace the O(n) linear scan at line 193.
- **Design:**
  ```
  class FenwickTree {
      std::vector<int> tree_;  // tree_[i] = partial sum of visible elements
  public:
      void insert(std::size_t pos, int val);   // insert element, shift tree
      void update(std::size_t pos, int delta); // toggle visibility (+1/-1)
      auto prefix_sum(std::size_t pos) const -> int;
      auto find_kth(int k) const -> std::size_t; // O(log n) binary lifting
      auto total() const -> int;
  };
  ```
- **Integration:** `ObjectState` gains `FenwickTree visibility_tree`. Updated by:
  - `list_insert()`: insert +1 at real_idx
  - `list_delete()`: update -1 at real_idx
  - `apply_op(insert/splice_text)`: insert +1 at rga_pos
  - `apply_op(del)`: update -1 at element position
- **Debug validation:** In debug builds, assert `find_kth` result matches linear scan.
- **Impact:** list/text operations 7-25x faster. This is the single highest-impact optimization.
- **Risk:** LOW-MEDIUM — well-understood data structure, validated against linear scan.

### 11A.5 — OpId-to-position index for RGA merge
- **File:** `src/doc_state.hpp`
- **What:** Add `std::unordered_map<OpId, std::size_t> element_index` to `ObjectState` mapping
  `insert_id → real_index`. Replace linear search in `find_rga_position` (line 380) with O(1)
  hash lookup. Maintain on insert (shift all indices >= insertion point).
- **Impact:** merge 1.5-2x faster, critical for large document merges.
- **Risk:** MEDIUM — index maintenance on vector insert requires careful handling.
- **Depends on:** 11A.4 (both modify list element management).

---

## Phase 11B: Memory & Allocation Optimizations

### 11B.1 — Bloom filter stack-allocated probe array
- **File:** `src/sync/bloom_filter.hpp`
- **What:** Change `get_probes()` return from `std::vector<uint32_t>` to
  `std::array<uint32_t, 7>`. Eliminates heap allocation per bloom filter check.
- **Impact:** 5-10% faster sync operations.
- **Risk:** LOW.

### 11B.2 — String copy elimination in Transaction
- **File:** `src/transaction.cpp`
- **What:** In `put()`, `put_object()`, `delete_key()`, `increment()`, `mark()` — create
  `std::string` from key once and move it through the call chain, instead of creating 3 copies
  per operation. Change `map_pred()`, `map_put()`, `map_delete()`, `counter_increment()` to
  accept `const std::string&` or take by move where the string is consumed.
- **Impact:** ~25% faster map_put (saves 2 heap allocations per put).
- **Risk:** LOW — pure refactoring.

### 11B.3 — Serialization buffer pre-sizing
- **Files:** `src/document.cpp`, `src/storage/change_chunk.hpp`, `src/storage/columns/raw_column.hpp`
- **What:** Add `reserve()` calls with size heuristics before building output buffers.
  E.g., `body.reserve(128 + change_history.size() * 256)` in `save()`.
  In `serialize_change_body()`: `body.reserve(64 + ops.size() * 32)`.
  In `write_raw_columns()`: `output.reserve(output.size() + total_column_bytes)`.
- **Impact:** save 1.5-1.7x faster, save_large 1.7x faster.
- **Risk:** LOW.

### 11B.4 — SHA-256 stack-allocated padding buffer
- **File:** `src/crypto/sha256.hpp`
- **What:** Replace heap-allocated `std::vector<std::byte>` padding with a stack buffer.
  Use `std::array<std::byte, 256>` for inputs up to 247 bytes (covers typical change hashing).
  Fall back to heap for larger inputs.
- **Impact:** 10-20% faster SHA-256, benefits all save/load/commit/sync paths.
- **Risk:** LOW — output verified by existing 7 NIST test vectors.

### 11B.5 — Reserve-based pre-allocation throughout
- **Files:** `src/document.cpp`, `src/doc_state.hpp`
- **What:** Add `reserve()` in `ops_to_patches()`, `build_actor_table()`, `all_change_hashes()`,
  `get_changes_since()`, `get_missing_deps()`, `get_changes_by_hash()`, `changes_visible_at()`,
  and other vector-building functions where size is known or estimable.
- **Impact:** 2-5% cumulative improvement.
- **Risk:** LOW.

### 11B.6 — Object pool for Op and ListElement
- **New file:** `src/pool.hpp`
- **What:** Implement a typed arena/object pool (`ObjectPool<T>`) that pre-allocates blocks of
  frequently created/destroyed objects. Use it for `Op` in the transaction hot path.
- **Design:**
  ```
  template <typename T, std::size_t BlockSize = 256>
  class ObjectPool {
      std::vector<std::unique_ptr<std::array<T, BlockSize>>> blocks_;
      std::vector<T*> free_list_;
  public:
      auto acquire() -> T*;        // O(1) amortized
      void release(T* obj);        // O(1)
      void reserve(std::size_t n); // pre-allocate n slots
  };
  ```
- **Usage in `Transaction`:** Instead of `pending_ops_.push_back(std::move(op))` allocating
  from the global heap on every operation, acquire from the pool. On `commit()`, ops move
  into the `Change` and pool slots are released.
- **Impact:** Eliminates per-operation heap allocation/deallocation churn. 15-30% faster for
  bulk transaction workloads (map_put_batch, text_splice_bulk).
- **Risk:** LOW-MEDIUM — pool is simple, integration touches hot paths. Pool owned by `DocState`.

---

## Phase 11C: Thread-Safe Document with Fork/Merge Parallelism

### Architecture

**Zero external threading dependencies.** No Taskflow, no TBB, no `std::execution::par`.
Built entirely from C++20 standard threading primitives (`std::jthread`, `std::shared_mutex`,
`std::latch`, `std::atomic`).

**Document is thread-safe** using `std::shared_mutex` — N concurrent readers, exclusive writers.

**Document constructor takes a thread count:**

```cpp
Document();                                   // default: hardware_concurrency()
explicit Document(unsigned int num_threads);   // explicit thread count
// num_threads == 0 → same as hardware_concurrency()
// num_threads == 1 → sequential, no pool, zero overhead
```

**Parallelism strategy: fork/merge.** Because CRDT merge is a monoid (associative,
commutative, idempotent), the Document can safely parallelize batch mutations by:

1. **Fork** N copies of the document (one per worker)
2. **Distribute** the work across forks (each fork processes a partition)
3. **Merge** all forks back into the original

The result is identical to sequential execution — this is guaranteed by the CRDT algebraic
properties. The internal thread pool (a simple `std::jthread` pool) avoids the overhead of
spawning/joining threads on every parallel operation.

**When fork/merge is used (write parallelism):**
- `transact()` with large batch operations (e.g., 1000 puts in a single transaction)
- `apply_changes()` with many independent changes
- Any mutation where work can be partitioned across independent document copies

**When direct parallelism is used (read-only work):**
- `save()` — serialize change bodies in parallel (each reads const data)
- `load()` — parse change chunks in parallel
- `ensure_hash_index()` — compute SHA-256 hashes in parallel
- DEFLATE compress/decompress — each z_stream is independent
- Bloom filter construction — atomic bit-setting

These read-only paths don't need fork/merge because they operate on const data with
independent outputs. The thread pool dispatches work chunks directly.

**Parallelism is only engaged when beneficial.** Below a threshold (e.g., <4 items to
process), the sequential path runs with zero overhead.

### 11C.0 — Remove Taskflow dependency
- **Files:** `CMakeLists.txt`, `src/executor.hpp`, `CLAUDE.md`
- **What:** Remove the Taskflow FetchContent block from CMakeLists.txt. Remove
  `Taskflow::Taskflow` from `target_link_libraries`. Delete `src/executor.hpp`.
  Update CLAUDE.md dependency list and any Taskflow references in docs.
- **Impact:** Faster builds (~15s saved on configure), no external threading dependency.
- **Risk:** NONE — Taskflow is linked but never called in any code path.

### 11C.1 — Internal thread pool (`src/thread_pool.hpp`)
- **New file:** `src/thread_pool.hpp`
- **What:** A minimal, header-only thread pool (~100 lines) built from `std::jthread`:
  ```cpp
  namespace automerge_cpp::detail {
  class ThreadPool {
      std::vector<std::jthread> workers_;
      // Task queue: std::mutex + std::deque<std::function<void()>>
      // Condition variable for waking idle workers
      std::atomic<bool> stop_{false};
  public:
      explicit ThreadPool(unsigned int num_threads);
      ~ThreadPool();  // sets stop_, notifies all, jthread auto-joins

      // Partition [0, count) into chunks and dispatch across workers.
      // Blocks until all chunks complete (std::latch).
      template <typename Fn>
      void parallel_for(std::size_t count, Fn&& fn);

      auto size() const -> unsigned int;
  };
  }  // namespace automerge_cpp::detail
  ```
  `parallel_for(N, fn)` divides `[0, N)` into `num_threads` chunks. Each worker calls
  `fn(i)` for its assigned range. Uses `std::latch` for completion. No work-stealing —
  the workloads (hash, serialize, compress) are uniform enough that static partitioning
  is sufficient.
- **Design rationale:** ~100 lines. No external dependencies. `std::jthread` auto-joins
  on destruction. Workers sleep on a condition variable when idle — zero CPU when unused.
- **Risk:** LOW — standard C++20 primitives only.

### 11C.2 — Thread-safe Document with constructor thread count
- **Files:** `include/automerge-cpp/document.hpp`, `src/doc_state.hpp`, `src/document.cpp`
- **What:**

  **Part A — Constructor with thread count:**
  ```cpp
  Document();                                   // hardware_concurrency() threads
  explicit Document(unsigned int num_threads);   // 0 = auto, 1 = sequential
  ```
  Document stores `std::shared_ptr<ThreadPool> pool_` (nullptr when `num_threads == 1`).
  `fork()` shares the same pool (shared_ptr copy). `load()` accepts optional thread count.

  **Part B — `std::shared_mutex` in DocState:**
  Add `mutable std::shared_mutex mutex_` to `DocState`. All existing `Document` methods
  acquire the appropriate lock:
  - **Shared lock (concurrent reads):** `get`, `get_all`, `keys`, `values`, `length`,
    `text`, `object_type`, `get_changes`, `get_heads`, `save`, `generate_sync_message`,
    `cursor`, `resolve_cursor`, `marks`, `actor_id`, all `*_at` methods.
  - **Exclusive lock (writes):** `set_actor_id`, `transact`, `transact_with_patches`,
    `merge`, `apply_changes`, `receive_sync_message`.

  Thread safety is always on (even with `num_threads == 1`). The internal parallelism
  paths check `pool_ != nullptr` before dispatching to the pool.

- **Impact:** Thread-safe Document. N readers, exclusive writers. Constructor overload.
- **Risk:** LOW — `shared_mutex` is well-understood.

### 11C.3 — Parallel save (change serialization + compression)
- **File:** `src/document.cpp`, `src/storage/change_chunk.hpp`
- **What:** In `Document::save()`, when `pool_` exists and `change_history.size() > 3`:
  ```cpp
  auto bodies = std::vector<std::vector<std::byte>>(changes.size());
  pool_->parallel_for(changes.size(), [&](std::size_t i) {
      bodies[i] = storage::serialize_change_body(changes[i], actor_table);
  });
  ```
  Each `serialize_change_body()` reads const data and writes to independent output.
  DEFLATE compression within each change body also runs in the worker's context.
- **Impact:** save with N changes: up to min(N, pool_size)x faster.
- **Risk:** LOW — each call is pure (const input, independent output).

### 11C.4 — Parallel load (chunk parsing + decompression)
- **File:** `src/document.cpp`
- **What:** In `Document::load()`, after reading chunk byte ranges, parse all change
  chunks in parallel:
  ```cpp
  pool->parallel_for(chunk_spans.size(), [&](std::size_t i) {
      changes[i] = storage::parse_change_chunk(chunk_spans[i], actor_table);
  });
  // Sequential: apply changes (CRDT correctness requires causal ordering)
  ```
  DEFLATE decompression within each chunk runs in the worker's context.
  `load()` is static — accepts optional `unsigned int num_threads` parameter.
- **Impact:** Load with N changes: up to min(N, pool_size)x faster for parsing phase.
- **Risk:** LOW.

### 11C.5 — Parallel SHA-256 hash computation
- **File:** `src/doc_state.hpp`
- **What:** When rebuilding the hash cache (e.g., after `load()` with many new changes),
  hash all new changes in parallel:
  ```cpp
  auto new_hashes = std::vector<ChangeHash>(new_count);
  pool_->parallel_for(new_count, [&](std::size_t i) {
      new_hashes[i] = compute_change_hash(change_history[start + i]);
  });
  // Then sequentially insert into cached_hash_index_
  ```
  Each SHA-256 computation is completely independent.
- **Impact:** For 1000 new changes on 30 cores: ~25-30x faster hash computation.
- **Risk:** LOW.

### 11C.6 — Parallel bloom filter construction
- **File:** `src/sync/bloom_filter.hpp`
- **What:** When building a bloom filter from many hashes, compute probe positions
  in parallel. Use `std::atomic<uint8_t>` for the bit array:
  ```cpp
  auto atomic_bits = std::vector<std::atomic<uint8_t>>(byte_count);
  pool_->parallel_for(hashes.size(), [&](std::size_t i) {
      auto probes = get_probes(hashes[i]);
      for (auto p : probes)
          atomic_bits[p >> 3].fetch_or(1u << (p & 7), std::memory_order_relaxed);
  });
  ```
- **Impact:** 10-25x for documents with 1000+ changes.
- **Risk:** LOW — relaxed atomic OR is sufficient.

### 11C.7 — Fork/merge parallelism for batch mutations
- **Files:** `src/document.cpp`, `src/transaction.cpp`
- **What:** When a transaction contains a large batch of independent mutations (e.g.,
  `map_put_batch`, bulk `apply_changes`), Document can internally:
  1. Fork N copies (one per worker thread)
  2. Partition the mutations across forks
  3. Execute each partition on a pool thread
  4. Merge all forks back sequentially

  ```cpp
  // Inside transact() when ops.size() > threshold and pool_ exists:
  auto forks = std::vector<Document>(pool_->size());
  for (auto& f : forks) f = this->fork();

  pool_->parallel_for(pool_->size(), [&](std::size_t worker) {
      auto [begin, end] = partition(ops, worker, pool_->size());
      forks[worker].transact([&](auto& tx) {
          for (auto i = begin; i < end; ++i)
              apply_op(tx, ops[i]);
      });
  });

  for (auto& f : forks) this->merge(f);
  ```

  This is correct because merge is a monoid — the result is identical to sequential
  execution regardless of partition or merge order.
- **Applies to:** `apply_changes()` with many independent changes, future batch APIs.
- **Impact:** Batch mutations scale with core count for large workloads.
- **Risk:** MEDIUM — fork/merge overhead (deep copy + merge) must exceed the parallel
  speedup. Only beneficial for large batches (>100 ops). Needs careful threshold tuning.

---

## Phase 11D: Hardware Acceleration

### 11D.1 — Hardware SHA-256 (ARM Crypto Extensions + x86 SHA-NI)
- **File:** `src/crypto/sha256.hpp`
- **What:** Add compile-time-detected hardware SHA-256 implementations:
  - **ARM Crypto Extensions:** NEON intrinsics (`vsha256hq_u32`, `vsha256h2q_u32`,
    `vsha256su0q_u32`, `vsha256su1q_u32`) for Apple Silicon / ARMv8.2+.
  - **x86 SHA-NI:** Intel SHA Extensions (`_mm_sha256rnds2_epu32`, `_mm_sha256msg1_epu32`,
    `_mm_sha256msg2_epu32`) for Intel Xeon (Ice Lake+) / AMD Zen.
  Keep software fallback for platforms without either extension (older x86, MSVC, FreeBSD).
- **Detection:**
  ```
  #if defined(__ARM_FEATURE_SHA2) || (defined(__aarch64__) && defined(__APPLE__))
    // ARM Crypto Extensions path
  #elif defined(__SHA__) && defined(__x86_64__)
    // x86 SHA-NI path
  #else
    // Software fallback
  #endif
  ```
- **Impact:** SHA-256 3-10x faster depending on platform.
  - ARM: 5-10x (Apple Silicon / ARMv8.2+)
  - x86: 3-5x (Intel Ice Lake+ / AMD Zen+; confirmed SHA-NI on Xeon Platinum 8358)
  Combined with 11C.6 (parallel hashing): multiplicative — 30 cores x 3-10x per core.
- **Linux profiling confirmed:** SHA-256 is 7-73% of cycles on Xeon (software impl).
  SHA-NI would reduce sync_full_round_trip by ~40-50% and get_at by ~60-70%.
- **Risk:** MEDIUM — platform-specific intrinsics, must maintain software fallback.
  Validated by existing 7 SHA-256 NIST test vectors.
- **Depends on:** 11B.4.

### 11D.2 — Cache-line alignment for ObjectState
- **File:** `src/doc_state.hpp`
- **What:** Add `alignas(64)` to `ObjectState` to prevent false sharing when multiple
  objects are accessed from different threads during parallel operations. Add `alignas(64)`
  to `FenwickTree` internal vector as well.
- **Impact:** 10-20% faster when objects are accessed in parallel (e.g., during parallel
  change application or parallel bloom filter probes that touch different objects).
- **Risk:** LOW — alignment only, no structural changes.
- **Depends on:** 11A.4.

---

## Phase 11E: Benchmark Expansion & Validation

### 11E.1 — Large document benchmarks
- **File:** `benchmarks/placeholder_benchmark.cpp`
- **What:** Add benchmarks for:
  - 10K/100K element lists (catches O(n^2) in Fenwick tree and element index)
  - 10K-key maps
  - Large merges (100+ changes from multiple actors)
  - Large save/load (1000-change documents)
  - Large sync (documents with significant divergence)
  - Parallel save throughput (how well does it scale with core count?)
  - Parallel load throughput

### 11E.2 — Multi-threaded benchmarks
- **File:** `benchmarks/placeholder_benchmark.cpp`
- **What:** Add benchmarks measuring:
  - N threads doing concurrent reads on a single Document
  - Parallel save vs sequential save (varying change count: 10, 100, 1000)
  - Parallel load vs sequential load
  - std::reduce(par) merge (100 docs) vs sequential reduce
  - std::transform(par) save/load (1000 docs) vs sequential
  - Object pool throughput (acquire/release cycles)

### 11E.3 — Before/after comparison
- **What:** Save v0.3.0 baseline as JSON, compare with v0.4.0 using
  `google-benchmark --benchmark_format=json`.

### 11E.4 — Regression verification
- **What:** All 274 existing tests pass after each sub-phase. Add debug-mode assertions to
  validate Fenwick tree against linear scan, hash cache against full rebuild, element index
  against linear search. Add thread safety tests for concurrent Document access.

---

## Implementation Order

| Step | Item | Effort | Cumulative Impact | Status |
|------|------|--------|-------------------|--------|
| 1 | 11A.1 Remove op_log | 10 min | 10-15% mutations | TODO |
| 2 | 11A.2 unordered_map | 10 min | +15-30% lookups | TODO |
| 3 | 11B.1 Bloom stack array | 10 min | +5-10% sync | TODO |
| 4 | 11B.2 String copy elim | 20 min | +25% map_put | TODO |
| 5 | 11A.3 Hash cache | 30 min | +22x get_at, +5.6x sync | **DONE** |
| 5b | 11A.3b Actor table cache | 20 min | +1.2x save_large | **DONE** |
| 6 | 11B.3 Serialization reserve | 15 min | +1.1x save | **DONE** |
| 7 | 11B.4 SHA-256 stack buf | 30 min | +10-20% hashing | **DONE** |
| 8 | 11B.5 Reserve throughout | 15 min | +2-5% cumulative | TODO |
| 9 | 11B.6 Object pool | 1 hr | +15-30% bulk mutations | TODO |
| 10 | 11A.4 Fenwick tree | 2 hr | +7-25x list/text | TODO |
| 11 | 11A.5 Element index | 1 hr | +1.5-2x merge | TODO |
| 12 | 11C.0 Remove Taskflow | 5 min | Faster builds | **DONE** |
| 13 | 11C.1 Thread pool (`std::jthread` + `std::latch`) | 1 hr | Foundation for parallelism | TODO |
| 14 | 11C.2 Thread-safe Document + constructor(num_threads) | 1.5 hr | N concurrent readers | TODO |
| 15 | 11C.3 Parallel save (change serialization + compress) | 45 min | Up to 30x save (many changes) | TODO |
| 16 | 11C.4 Parallel load (chunk parsing + decompress) | 45 min | Up to 30x load parsing | TODO |
| 17 | 11C.5 Parallel SHA-256 hashing | 30 min | Up to 30x hash computation | TODO |
| 18 | 11C.6 Parallel bloom construction | 30 min | +10-25x bloom for large docs | TODO |
| 19 | 11C.7 Fork/merge for batch mutations | 1 hr | Batch mutations scale with cores | TODO |
| 20 | 11D.1 Hardware SHA-256 (ARM + x86) | 2 hr | +3-10x per-core SHA-256 | TODO |
| 21 | 11D.2 Cache alignment | 30 min | +10-20% parallel access | TODO |
| 22 | 11E.1-4 Benchmarks + tests | 1.5 hr | Validation | TODO |

---

## Parallelism Map: What Runs on 30 Cores

All parallelism uses the Document's internal `ThreadPool` (`std::jthread`-based).
No external dependencies. When `num_threads == 1`, all paths are sequential.

| Operation | Sequential Part | Parallel Part (thread pool) | Max Speedup (30 cores) |
|-----------|----------------|---------------------------|----------------------|
| `save()` | Header assembly | `parallel_for` change bodies (11C.3) | 25-30x |
| `save()` (per change) | Op loop for encoders | `parallel_for` column compress (11C.4) | 14x |
| `load()` | Header parse | `parallel_for` chunk parsing (11C.5) | 25-30x |
| `load()` (per chunk) | Column decode | `parallel_for` decompress (11C.4) | 14x |
| `load()` | — | `parallel_for` hash cache rebuild (11C.6) | 25-30x |
| `ensure_hash_index()` | — | `parallel_for` SHA-256 (11C.6) | 25-30x |
| `generate_sync_message()` | BFS traversal | `parallel_for` bloom build (11C.7) | 10-25x |
| SHA-256 (per call) | — | ARM/x86 intrinsics (11D.1) | 3-10x per core |

**Combined example — save a 1000-change document on 30 cores:**
- Sequential: serialize 1000 changes x (column encode + compress) = ~1000 x 1ms = 1s
- Parallel: 1000 changes / 30 cores = ~34 batches x 1ms = 34ms → **~30x speedup**

**Combined example — load a 1000-change compressed document on 30 cores:**
- Parallel parse: `parallel_for` on 1000 chunks → **~30x faster parsing**
- Sequential apply: unchanged (CRDT correctness requires causal ordering)
- Parallel hash cache: `parallel_for` on 1000 hashes → **~30x faster hashing**

---

## New Files

| File | Description |
|------|-------------|
| `src/thread_pool.hpp` | Internal `std::jthread`-based thread pool with `parallel_for` / `parallel_transform` |
| `src/pool.hpp` | Typed object pool for Op, ListElement |

---

## Projected Results (v0.3.0 → v0.4.0)

### macOS (Apple M3 Max, 16 cores)

| Benchmark | v0.3.0 | v0.4.0 (est.) | Speedup |
|-----------|--------|---------------|---------|
| map_put (single) | 953 K ops/s | ~1.4 M ops/s | 1.5x |
| map_put_batch/1000 | 3.44 M ops/s | ~5.5 M ops/s | 1.6x |
| map_get | 28.5 M ops/s | ~50 M ops/s | 1.8x |
| list_insert_append | 15.2 K ops/s | ~200 K ops/s | 13x |
| list_insert_front | 14.6 K ops/s | ~200 K ops/s | 14x |
| list_get (1000 elem) | 4.48 M ops/s | ~33 M ops/s | 7x |
| text_splice_bulk (100 chars) | 27 K chars/s | ~670 K chars/s | 25x |
| save (100 keys, 1 change) | 10.4 K ops/s | ~18 K ops/s | 1.7x |
| load (100 keys, 1 change) | 38.2 K ops/s | ~67 K ops/s | 1.7x |
| save_large (1000 items) | 1.38 KiB/s | ~35 KiB/s | 25x |
| merge (10+10 puts) | 248.7 K ops/s | ~500 K ops/s | 2x |
| sync_full_round_trip | 4.28 K ops/s | ~20 K ops/s | 4.7x |
| cursor_resolve | 6.04 M ops/s | ~40 M ops/s | 6.6x |

### Linux (Intel Xeon Platinum 8358, 30 cores)

| Benchmark | v0.3.0 (baseline) | v0.4.0 (measured) | Speedup | v0.4.0 (est.) | Total Speedup |
|-----------|-------------------|-------------------|---------|---------------|---------------|
| map_put (single) | 876 K ops/s | 922 K ops/s | 1.05x | ~1.3 M ops/s | 1.5x |
| map_put_batch/1000 | 3.05 M ops/s | 3.52 M ops/s | 1.15x | ~4.9 M ops/s | 1.6x |
| map_get | 27.1 M ops/s | 27.2 M ops/s | 1.0x | ~49 M ops/s | 1.8x |
| list_insert_append | 7.1 K ops/s | 6.3 K ops/s | 1.0x | ~92 K ops/s | 13x |
| list_insert_front | 13.5 K ops/s | 13.5 K ops/s | 1.0x | ~189 K ops/s | 14x |
| list_get (1000 elem) | 2.78 M ops/s | 2.75 M ops/s | 1.0x | ~19 M ops/s | 7x |
| text_splice_bulk (100 chars) | 9.3 K chars/s | 9.7 K chars/s | 1.0x | ~232 K chars/s | 25x |
| save (100 keys, 1 change) | 28.8 K ops/s | **31.8 K ops/s** | **1.1x** | ~54 K ops/s | 1.9x |
| load (100 keys, 1 change) | 27.6 K ops/s | 27.7 K ops/s | 1.0x | ~47 K ops/s | 1.7x |
| save_large (1000 items) | 2.4 KiB/s | **2.4 KiB/s** | **1.2x** | ~10 KiB/s | 4x |
| merge (10+10 puts) | 258 K ops/s | 260 K ops/s | 1.0x | ~516 K ops/s | 2x |
| sync_full_round_trip | 4.7 K ops/s | **26.4 K ops/s** | **5.6x** | ~40 K ops/s | 8.5x |
| sync_generate_message | 1.50 M ops/s | **4.16 M ops/s** | **2.8x** | ~6 M ops/s | 4x |
| get_at | 130 K ops/s | **2.88 M ops/s** | **22.1x** | ~3.5 M ops/s | 27x |
| text_at | 425 K ops/s | **844 K ops/s** | **2.0x** | ~5 M ops/s | 12x |
| cursor_resolve | 2.04 M ops/s | **2.90 M ops/s** | **1.4x** | ~13.5 M ops/s | 6.6x |
| **NEW: parallel save (1000 changes, 30 cores)** | N/A | — | — | benchmark | — |
| **NEW: parallel load (1000 changes, 30 cores)** | N/A | — | — | benchmark | — |
| **NEW: concurrent reads (Document, 120 threads)** | N/A | — | — | benchmark | — |
| **NEW: std::reduce(par) merge (100 docs, 30 cores)** | N/A | — | — | benchmark | — |

**v0.4.0 optimizations completed:** 11A.3 (hash cache), 11A.3b (actor table cache),
11B.3 (buffer pre-sizing), 11B.4 (SHA-256 stack buffer). Remaining v0.4.0 estimates
assume Fenwick tree (11A.4), hardware SHA-256 (11D.1), and parallelism (11C).

---

## Verification

After each sub-phase:
```bash
cmake --build build && ctest --test-dir build --output-on-failure
```

After all phases, run release benchmarks:
```bash
cmake -B build-release -DCMAKE_BUILD_TYPE=Release \
    -DAUTOMERGE_CPP_BUILD_BENCHMARKS=ON \
    -DAUTOMERGE_CPP_BUILD_TESTS=ON
cmake --build build-release
./build-release/benchmarks/automerge_cpp_benchmarks
```

Compare against v0.3.0 baseline numbers documented in `docs/benchmark-results.md`.

---

## Critical Files

| File | Optimizations |
|------|--------------|
| `src/doc_state.hpp` | 11A.1-5, 11B.5, 11C.2, 11C.5, 11D.2 (central target) |
| `src/transaction.cpp` | 11A.1, 11B.2, 11B.6, 11C.7 |
| `src/document.cpp` | 11A.1, 11B.3, 11B.5, 11C.2-5, 11C.7 |
| `include/automerge-cpp/document.hpp` | 11C.2 (constructor + thread count) |
| `src/thread_pool.hpp` | 11C.1 (NEW) |
| `src/crypto/sha256.hpp` | 11B.4, 11D.1 |
| `src/storage/change_chunk.hpp` | 11B.3, 11C.3, 11C.4 |
| `src/storage/columns/change_op_columns.hpp` | 11C.3 |
| `src/sync/bloom_filter.hpp` | 11B.1, 11C.6 |
| `src/storage/columns/raw_column.hpp` | 11B.3 |
| `src/pool.hpp` | 11B.6 (NEW) |
| `CMakeLists.txt` | 11C.0 (remove Taskflow) |
| `benchmarks/placeholder_benchmark.cpp` | 11E.1-2 |
| `docs/plans/roadmap.md` | Update with Phase 11 |
